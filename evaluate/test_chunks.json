{
    "1": [
        {
            "url": "https://docs.nersc.gov/development/build-tools/spack/#need-help-with-spack",
            "content": "## Need Help with Spack\n\nIf you need help with Spack, please join the [Spack Slack Channel](https://slack.spack.io/), once you have registered you\ncan go to slack workspace at https://spackpm.slack.com/.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/development/programming-models/sycl/#support",
            "content": "## Support\n\n* `#sycl` channel in [NERSC Users\n  Slack](https://www.nersc.gov/users/NUG/nersc-users-slack/) (login\n  required)\n* [NERSC Help Desk](https://help.nersc.gov)",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/#other-nersc-web-pages",
            "content": "## Other NERSC web pages\n\n* [NERSC Home page](https://nersc.gov) - NERSC news and information\n* [MOTD](https://www.nersc.gov/live-status/motd/) - Live status of NERSC services\n* [MyNERSC](https://my.nersc.gov) - Interactive content\n* [Help Portal](https://help.nersc.gov) - Open support tickets, make resource requests\n* [Jupyter](https://jupyter.nersc.gov) - Access NERSC with interactive notebooks and more\n* [Iris](https://iris.nersc.gov) - NERSC account management\n\n!!! tip\n    The [NERSC Users Group (NUG)](https://www.nersc.gov/users/NUG/) is an\n    independent organization of users of NERSC resources.\n\n    All users are welcome to [join the NUG Slack\n    workspace](https://www.nersc.gov/users/NUG/nersc-users-slack/).\n\n!!! info \"NERSC welcomes your contributions\"\n    This project is hosted on [GitLab](https://gitlab.com/NERSC/nersc.gitlab.io) and your\n    [contributions](https://gitlab.com/NERSC/nersc.gitlab.io/blob/main/CONTRIBUTING.md)\n    are welcome!",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/applications/wrf/#wrf-special-interest-group-wrf-sig-at-nersc",
            "content": "## WRF Special Interest Group (WRF-SIG) at NERSC\n\nWRF-SIG is a group of WRF users at NERSC. We have quasi-regular meetings at every few months to \nexchange information and learn technical and scientific issues related to the WRF model and NERSC \nsystems. The members will have access to the shared data directory and support tickets issued by \nother members to be aware of on-going technical issues for WRF at NERSC.\n\nPlease join the group through\n[this google form](https://docs.google.com/forms/d/e/1FAIpQLSeu76SWSzUE9SSQhmbckB9WkBgpxD307YSmpFpv4_fsYvGzAg/viewform) \nand also subscribe to the\n[Slack channel](https://nerscusers.slack.com/archives/C03E4P8NU7M) \n(it is required to first join\n[the NERSC Users Slack workspace](https://www.nersc.gov/users/NUG/nersc-users-slack/)).",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/services/spin/examples/#community-showcase",
            "content": "## Community Showcase\n\nThis section highlights approaches or techniques developed by Spin users.\n\nIf you'd like to share something you've done with Spin that you think\nother users would find helpful, we'd love to add your work to this\nshowcase! Contact us by [submitting a ticket](https://help.nersc.gov/).\n\nYou can also find other Spin users in the #spin channel of the\n[NERSC Users Slack workspace](https://www.nersc.gov/users/NUG/nersc-users-slack/).",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/services/jupyter/#getting-help-with-jupyter",
            "content": "## Getting Help with Jupyter\n\nIf you run into problems with Jupyter at NERSC or just have questions about how\nto use it, please use the\n[NERSC Help Portal](https://help.nersc.gov) to open a ticket and we will get\nback to you as soon as possible.\nIn that case, before opening your ticket you may want to\n[examine your Jupyter logs](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/services/jupyter/reference/#jupyter-server-logs).\n\nYou may also find the #jupyter channel on the\n[NERSC User Group Slack](https://www.nersc.gov/users/NUG/nersc-users-slack/)\nto be a useful resource where fellow users share knowledge, experiences, and\nadvice.\nBut again, to get help from NERSC staff, use the [Help Portal](https://help.nersc.gov).",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/iris/iris-for-pis/#reviewing-a-user-request-to-join-a-project",
            "content": "### Reviewing a User Request to Join a Project\n\nOnce the user has submitted a request and their account has completed \nthe compliance vetting, PIs and PI Proxies (that is, the project managers\nwith the 'Accept Users' permission) will be notified about the\nsubmission.  Any of them may then log into Iris and approve the\naccount request, following the steps below.\n\n1.  Select the project from the 'Projects' pull-down menu on the\n    top menu bar.\n2.  Select the 'Roles' tab.\n3.  You will see the pending requests in the 'Pending Members'\n    table (you may need to scroll down the page).  Review the information \n    provided by the user to be sure they are a valid user who needs an \n    account under your project.\n\n    ![CPU tab Allocation Transfer report link](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/iris/images/iris_projects_roles_pendingmembers.png)\n    {: align=\"center\" style=\"border:1px solid black\"}\n\n4.  If everything looks OK, click on the <span style=\"color:green\">**green**</span> Thumbs Up button.\n    In the dialog box that appears:\n    -   Set the project role for the user: `user`,\n        `project_resource_manager`, `project_membership_manager`,\n\tor `pi_proxy`.\n        - NOTE: There can be only one user with the PI role per project.\n    -   Set CPU/GPU Node Hour quotas in either the 'Allocated",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/development/programming-models/openmp/#what-is-openmp",
            "content": "one node or socket, NUMA across nodes or sockets, and the typical hybrid\nprogramming model of hybrid MPI/OpenMP). The current architecture trend needs a\nhybrid programming model with three levels of parallelism: MPI between nodes or\nsockets, shared memory (such as OpenMP) on the nodes/sockets, and increased\nvectorization for lower-level loop structures.\n\nOpenMP has three components: compiler directives and clauses, runtime\nlibraries, and environment variables. The compiler directives are only\ninterpreted when the OpenMP compiler option is turned on. OpenMP uses the \"fork\nand join\" execution model: the master thread forks new threads at the beginning\nof parallel regions, multiple threads share work in parallel; and threads join\nat the end of parallel regions.\n\n<img style=\"float: center;\" alt=\"OpenMP fork and join model\" src=\"./OpenMPforkjoin.png\">\n\nIn OpenMP, all threads have access to the same shared global memory. Each\nthread has access to its own private local memory. Threads synchronize\nimplicitly by reading and writing shared variables. No explicit communication\nis needed between threads.\n\n\nThe thread that executes the implicit parallel region that surrounds\nthe whole program executes on the host device. OpenMP supports other\ndevices (e.g., GPUs) besides the host device (i.e., CPUs). On\nPerlmutter, GPUs are available to the host device for offloading\ncode and data. Each device has its own threads that are distinct\nfrom threads that execute on another device, and threads cannot\nmigrate from one device to another device. For info on how to offload\ncode and data, please see the",
            "is_markdown": true
        }
    ],
    "2": [
        {
            "url": "https://docs.nersc.gov/#other-nersc-web-pages",
            "content": "## Other NERSC web pages\n\n* [NERSC Home page](https://nersc.gov) - NERSC news and information\n* [MOTD](https://www.nersc.gov/live-status/motd/) - Live status of NERSC services\n* [MyNERSC](https://my.nersc.gov) - Interactive content\n* [Help Portal](https://help.nersc.gov) - Open support tickets, make resource requests\n* [Jupyter](https://jupyter.nersc.gov) - Access NERSC with interactive notebooks and more\n* [Iris](https://iris.nersc.gov) - NERSC account management\n\n!!! tip\n    The [NERSC Users Group (NUG)](https://www.nersc.gov/users/NUG/) is an\n    independent organization of users of NERSC resources.\n\n    All users are welcome to [join the NUG Slack\n    workspace](https://www.nersc.gov/users/NUG/nersc-users-slack/).\n\n!!! info \"NERSC welcomes your contributions\"\n    This project is hosted on [GitLab](https://gitlab.com/NERSC/nersc.gitlab.io) and your\n    [contributions](https://gitlab.com/NERSC/nersc.gitlab.io/blob/main/CONTRIBUTING.md)\n    are welcome!",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/services/jupyter/#getting-help-with-jupyter",
            "content": "## Getting Help with Jupyter\n\nIf you run into problems with Jupyter at NERSC or just have questions about how\nto use it, please use the\n[NERSC Help Portal](https://help.nersc.gov) to open a ticket and we will get\nback to you as soon as possible.\nIn that case, before opening your ticket you may want to\n[examine your Jupyter logs](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/services/jupyter/reference/#jupyter-server-logs).\n\nYou may also find the #jupyter channel on the\n[NERSC User Group Slack](https://www.nersc.gov/users/NUG/nersc-users-slack/)\nto be a useful resource where fellow users share knowledge, experiences, and\nadvice.\nBut again, to get help from NERSC staff, use the [Help Portal](https://help.nersc.gov).",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/getting-started/#how-to-file-a-good-ticket",
            "content": "### How to File a Good Ticket\n\nNERSC Consultants handle thousands of support requests per\nyear. In order to ensure efficient timely resolution of your issue,\ninclude **as much of the following as possible** when making a\nrequest:\n\n* error messages\n* jobids\n* location of relevant files\n    * input/output\n    * job scripts\n    * source code\n    * executables\n* output of `module list`\n* any steps you have tried\n* steps to reproduce\n\nPlease copy and paste any text directly into the ticket and only\ninclude screenshots as attachements when the graphical output is\nthe subject of the support request.\n\n!!! tip\n    You can make code snippets, shell outputs, etc in your ticket much more\n    readable by inserting a line with:\n\n    ```\n    [code]<pre>\n    ```\n\n    before the snippet, and another line with:\n\n    ```\n    </pre>[/code]\n    ```\n\n    after it. While these are the most useful, other options to improve\n    formatting can be found in the [full list of formatting\n    options](https://community.servicenow.com/community?id=community_blog&sys_id=4d9ceae1dbd0dbc01dcaf3231f9619e1).\n\nAccess to the online help system requires logging in with your NERSC username,\npassword, and one-time password. If you are an existing user unable to log in,\nyou can send an email to <accounts@nersc.gov> for support.\n\nIf you are not a NERSC user, you can reach NERSC with your queries at\n`accounts@nersc.gov` or `allocations@nersc.gov`.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/performance/io/#tutorials-support-and-resource-allocation",
            "content": "## Tutorials, Support, and Resource Allocation\n\nHere, we list additional support resources for NERSC users, as well as pointers to previous\n and ongoing research projects associated with NERSC staff and LBL researchers to support high-performance scientific I/O.\n\n\n### Online tutorials at NERSC\n\n+ A brief overview of [I/O Formats](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/performance/io/library/) at in use NERSC\n(focused on MPI I/O and HDF5)\n\n\n### User support at NERSC\n\n+ [Consulting services](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/getting-started/#getting-help) provided by NERSC Consultants\n\n\n### Previous and ongoing I/O research projects contributed to by NERSC and LBL researchers\n\n+ The [ExaHDF5](https://sdm.lbl.gov/exahdf5/) group is working to\ndevelop next-generation I/O libraries and middleware to support\nscientific I/O (focused in particular on the HDF5 data format)",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/iris/iris-for-users/#resources",
            "content": "## Resources\n\nThis section contains links to some popular NERSC resource pages:\n\n-   [Technical documentation webpage](https://docs.nersc.gov/)\n-   [NERSC trouble ticket system](https://help.nersc.gov)\n-   [Jupyter](https://jupyter.nersc.gov/)\n\n---",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/jobs/workflow/community_tools/#community-supported-tools",
            "content": "# Community Supported Tools\n\nWe encourage contributions from workflow tool developers \nand the NERSC user community on this page. NERSC staff may not be \nfamiliar with all workflow tools presented here, so we encourage \nyou to reach out directly to the tool developers for questions and support.\nTo help us better understand which tools you are using or interested in, \nplease don't hesitate to open a ticket at [help.nersc.gov](https://help.nersc.gov). \nThis information is helpful for NERSC staff to gauge how best to support users.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/services/globus/#guest-collections",
            "content": "system](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/filesystems/dna/) for JGI users).\n\nTo share data, create a `gsharing` directory in your project's\ndirectory on the Community File System. Then [open a\nticket](https://help.nersc.gov/) and let us know the directory path\nyou'd like to share (we currently have to manually update the\nconfiguration file to allow directories to be shared).\n\nOnce you hear back from us, go to the [NERSC Share Globus endpoint web\npage](https://app.globus.org/file-manager/collections/b6534bbc-5bb1-11e9-bf33-0edbf3a4e7ee/overview).\nClick on the \"Collections\" tab and select \"Add a Guest\nCollection\". This will bring you to a screen where you can give your\nshared endpoint a name and fill in the path you'd like to share. Most\npeople choose to create a subdirectory in the `gsharing` directory and\nshare that. If you had multiple collections of data you wanted to\nshare with different sets of people, you could create different\n\nClick on the \"Create Share\" button and it will take you to another\nscreen where you can share this endpoint with specific Globus users or\nwith all Globus users (these users do **not** have to be NERSC\nusers). You can also make other Globus users administrators, which\nwill mean that they will have the power to add or remove other users\nfrom the shared endpoint.\n\n!!! warning\n    If your files and directories are set to be world readable, they",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/getting-started/#help-desk",
            "content": "### Help Desk\n\n!!! info \"Availability\"\n\tAccount support is available 8-5 Pacific Time on business days.\n\nThe [online help desk](https://help.nersc.gov/) is the **preferred**\nmethod for contacting NERSC.\n\n!!! help \"Before you open a ticket\"\n    * [Password resets](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/accounts/passwords/) can be done automatically\n       without opening a ticket!\n    * We encourage you to search [our documentation](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/) before\n      opening a ticket.\n    * New users should read this [Getting started guide](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/getting-started/).\n    * Account changes can be processed through\n      [Iris](https://iris.nersc.gov) ([Iris guide for\n      users](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/iris/iris-for-users/)).",
            "is_markdown": true
        }
    ],
    "3": [
        {
            "url": "https://docs.nersc.gov/jobs/#writing-a-job-script",
            "content": "#### Writing a Job Script\n\nA clear job script will include at least the number of nodes,\nwalltime, type of nodes (constraint), quality of service (QOS),\nand for Perlmutter GPU jobs, the number of GPUs. These options could be\nspecified on the command line, but for clarity and to establish a\nrecord of the job submission, we recommend including all these options\n(and more) in your job script.\n\nA Slurm job script begins with a shell invocation (e.g.,\n`#!/bin/bash`) followed by lines of directives, each of which begins\nwith `#SBATCH`.  After these directives, users then include the\ncommands to be run in the script, including the setting of environment\nvariables and the setup of the job. Usually (but not always) the\nscript includes at least one `srun` command, launching a parallel job\nonto one or more nodes allocated to the job.\n\n```slurm\n#!/bin/bash\n#SBATCH --nodes=<nnodes>\n#SBATCH --time=hh:mm:ss\n#SBATCH --constraint=<architecture>\n#SBATCH --qos=<QOS>\n#SBATCH --account=<project_name>\n\n# set up for problem & define any environment variables here\n\nsrun -n <num_mpi_processes> -c <cpus_per_task> a.out\n\n# perform any cleanup or short post-processing here\n```\n\nThe above script is easily applied only to the simplest of cases and\nis not widely generalizable. In this simple case, a user would replace\nthe items between `< >` with specific arguments, e.g., `--nodes=2` or\n`--qos=debug`. The format for the maximum walltime request is number",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/jobs/policy/#perlmutter-gpu",
            "content": "* GPU jobs in the `shared` QOS may request 1 or 2 GPUs and will be allocated\n   a corresponding 16 CPU cores and 64 GB RAM or 32 CPU cores and 128 GB RAM,\n   respectively.\n\n  * NERSC's Jupyter service uses the `jupyter` QOS to start JupyterLab\n    on compute nodes.  Other uses of the QOS are currently not \n    authorized, and the QOS is monitored for unauthorized use.\n\n * Jobs in the \"preempt\" QOS must request a minimum of 2 hours of walltime, and\n   these jobs are charged for a minimum of 2 hours of walltime. Preemptible \n   jobs are subject to preemption after two hours. Jobs can be automatically\n   [Preemptible Jobs section](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/jobs/examples/#preemptible-jobs) for details.\n\n * Jobs may run on the \"standard\" Perlmutter GPU nodes or on the subset of GPU\n   nodes which have double the GPU-attached memory. To specifically request\n   these higher-bandwidth memory nodes, use  `-C gpu&hbm80g` in your job script\n   instead of `-C gpu`. Jobs with this constraint must use 256 or fewer nodes.\n   To specifically request the \"standard\" Perlmutter GPU nodes, use \n   `-C gpu&hbm40g` in your job script.\n\n!!! warning \"Specific GPU requests on the command line\"",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/jobs/policy/#perlmutter-gpu",
            "content": "* Jobs in the \"preempt\" QOS must request a minimum of 2 hours of walltime, and\n   these jobs are charged for a minimum of 2 hours of walltime. Preemptible \n   jobs are subject to preemption after two hours. Jobs can be automatically\n   [Preemptible Jobs section](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/jobs/examples/#preemptible-jobs) for details.\n\n * Jobs may run on the \"standard\" Perlmutter GPU nodes or on the subset of GPU\n   nodes which have double the GPU-attached memory. To specifically request\n   these higher-bandwidth memory nodes, use  `-C gpu&hbm80g` in your job script\n   instead of `-C gpu`. Jobs with this constraint must use 256 or fewer nodes.\n   To specifically request the \"standard\" Perlmutter GPU nodes, use \n   `-C gpu&hbm40g` in your job script.\n\n!!! warning \"Specific GPU requests on the command line\"\n    which uses one of the memory-specific GPU constraints, you will need to\n    specify the constraint within quotation marks (e.g., `-C \"gpu&hbm80g\"`)\n    so that the ampersand in the constraint is not interpreted as a shell\n    symbol.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/connect/vscode/#running-on-a-compute-node",
            "content": "When connecting you'll first connect to a Perlmutter login node and request and \ninteractive allocation from Slurm with `salloc`. The following `salloc` line \nrequests one GPU node for 60 minutes. Remember to change `m0000`` to the account \nthat you want to charge hours from.\n\n```\nelvis@perlmutter-login34[~]$ salloc --nodes 1 --qos interactive --time \n00:60:00 -C gpu -A m0000\nsalloc: Pending job allocation 19622394\nsalloc: job 19622394 queued and waiting for resources\nsalloc: Granted job allocation 19622394\nsalloc: Waiting for resource configuration\nsalloc: Nodes nid200021 are ready for job\nelvis@perlmutter-nid200021[~]$\n```\n\nOnce you have an allocation copy the the node name starting with `nid` and go \nwith `nid` as the hostname you want to connect to.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/jobs/interactive/#perlmutter-debug-qos",
            "content": "#### Perlmutter \"debug\" QoS\n\nTo get access to CPU node, you can run the following command:\n\n```\nsalloc --nodes 1 --qos debug --time 20:00 --constraint cpu --account=mxxxx\n```\n\nFor GPU node, you can run: \n\n```\nsalloc --nodes 1 --qos debug --time 20:00 --constraint gpu --account=mxxxx\n```",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/applications/vasp/examples/long-time.sh",
            "content": "#SBATCH --time-min=0:05:00\n#SBATCH --signal=B:USR1@60\n#SBATCH --requeue\n#SBATCH --open-mode=append\n\n## Notes on parameters above:\n##\n## '--qos=XX'      can be set to any of several QoS to which the user has access, which\n##                 may include: regular, debug, shared, preempt, debug-preempt, premium,\n##                 overrun, or shared-overrun.\n## '--comment=XX'  is the total time that SUM of restarts can run (can be VERY LARGE).\n## '--time=XX'     is the maximum time that individual restart can run. This MUST fit\n##                 inside the queue limit for the QoS that you want to use\n##                 (see https://docs.nersc.gov/jobs/policy/ for details).\n## '--time-min=XX' is the minimum time that job can run before being preempted (using this\n##                 option can make it easier for Slurm to fit into queue, possibly allowing\n##                 job to start sooner). Omit this parameter unless running in either\n##                 --qos=preempt or --qos=debug_preempt.\n##                 (set this large enough to have enough time to write checkpoint file(s)\n##                  before time limit is reached; 60 seconds is usually enough).\n## '--requeue'     specifies job is elegible for requeue in case of preemption.\n## '--open-mode=append' appends contents of to the end of standard output and standard\n##                 error files with successive requeues.\n\n# Remove STOPCAR file so job isn't blocked",
            "is_markdown": false
        },
        {
            "url": "https://docs.nersc.gov/applications/vasp/examples/long-time.sh",
            "content": "##                 overrun, or shared-overrun.\n## '--comment=XX'  is the total time that SUM of restarts can run (can be VERY LARGE).\n## '--time=XX'     is the maximum time that individual restart can run. This MUST fit\n##                 inside the queue limit for the QoS that you want to use\n##                 (see https://docs.nersc.gov/jobs/policy/ for details).\n## '--time-min=XX' is the minimum time that job can run before being preempted (using this\n##                 option can make it easier for Slurm to fit into queue, possibly allowing\n##                 job to start sooner). Omit this parameter unless running in either\n##                 --qos=preempt or --qos=debug_preempt.\n##                 (set this large enough to have enough time to write checkpoint file(s)\n##                  before time limit is reached; 60 seconds is usually enough).\n## '--requeue'     specifies job is elegible for requeue in case of preemption.\n## '--open-mode=append' appends contents of to the end of standard output and standard\n##                 error files with successive requeues.\n\n# Remove STOPCAR file so job isn't blocked\nrm STOPCAR\nfi\n\n# Select VASP module of choice\nmodule load vasp/5.4.4-cpu\n\n# srun must execute in background and catch signal on wait command\n# so ampersand ('&') is REQUIRED here\nsrun -n 128 -c 2 --cpu_bind=cores vasp_std &\n\n# Put any commands that need to run to continue the next job (fragment) here\nckpt_vasp() {",
            "is_markdown": false
        },
        {
            "url": "https://docs.nersc.gov/applications/vasp/examples/long-time.sh",
            "content": "#!/bin/bash\n#SBATCH -N 1\n#SBATCH -C cpu\n#SBATCH -J vasp_job\n#SBATCH -o %x-%j.out\n#SBATCH -e %x-%j.err\n#SBATCH --qos=debug_preempt\n#SBATCH --comment=0:45:00\n#SBATCH --time=0:05:00\n#SBATCH --time-min=0:05:00\n#SBATCH --signal=B:USR1@60\n#SBATCH --requeue\n#SBATCH --open-mode=append\n\n## Notes on parameters above:\n##\n## '--qos=XX'      can be set to any of several QoS to which the user has access, which\n##                 may include: regular, debug, shared, preempt, debug-preempt, premium,\n##                 overrun, or shared-overrun.\n## '--comment=XX'  is the total time that SUM of restarts can run (can be VERY LARGE).\n## '--time=XX'     is the maximum time that individual restart can run. This MUST fit\n##                 inside the queue limit for the QoS that you want to use\n##                 (see https://docs.nersc.gov/jobs/policy/ for details).\n## '--time-min=XX' is the minimum time that job can run before being preempted (using this\n##                 option can make it easier for Slurm to fit into queue, possibly allowing\n##                 job to start sooner). Omit this parameter unless running in either\n##                 --qos=preempt or --qos=debug_preempt.",
            "is_markdown": false
        }
    ],
    "4": [
        {
            "url": "https://docs.nersc.gov/development/checkpoint-restart/dmtcp/#perlmutter",
            "content": "3. There is only one job id, and one standard output/error file\n   associated with multiple requeued jobs. The Slurm `sacct` command accepts a\n   `--duplicates` flag which can be used to display more complete information about requeued\n   jobs.\n\nThese features are enabled with the following additional `sbatch` flags\nand a bash function `requeue_job`, which traps the signal (USR1) sent\nfrom Slurm:\n\n#SBATCH --comment=12:00:00         #maximum time available to job and all requeued jobs\n#SBATCH --signal=B:USR1@60\n#SBATCH --requeue                  #specify job is requeueable\n#SBATCH --open-mode=append         #to append standard out/err of the requeued job  \n                                   #to that of the previously terminated job\n```\n  \n```shell\n#requeueing the job if remaining time >0\nrequeue_job func_trap USR1\n\nwait\n```\n\nwhere the `--comment` sbatch flag is used to specify the desired\nwalltime and to track the remaining walltime for the job (after\npre-termination).  You can specify any length of time, e.g., a week or\neven longer.  The `--signal` flag is used to request that the batch\nsystem sends user-defined signal USR1 to the batch shell (where the\njob is running) `sig_time` seconds (e.g., 60) before the job hits the\nwall limit.  i\n\nUpon receiving the signal USR1 from the batch system 60 seconds before\nthe job hits the wall limit, the `requeue_job` executes the following",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/development/checkpoint-restart/mana/#perlmutter-cpu",
            "content": "the desired walltime (48 hours in this example).   \n\n2. Optionally, each job checkpoints one more time 300 seconds before the\n   job hits the allocated time limit.   \n\n3. There is only one job ID, and one standard output/error file\n   associated with multiple shorter jobs.\n\nThese features are enabled with the following additional sbatch flags\nfrom the batch system:\n\n```slurm\n#SBATCH --comment=48:00:00         #comment for the job\n#SBATCH --signal=B:USR1@<sig_time> \n#SBATCH --requeue                  #specify job is requeueable\n#SBATCH --open-mode=append         #to append standard out/err of the requeued job  \n                                   #to that of the previously terminated job\n```\n  \n```shell\n#requeueing the job if remaining time >0\nckpt_command=ckpt_mana \n\nwait\n```\n\nwhere the `--comment` sbatch flag is used to specify the desired\nwalltime and to track the remaining walltime for the job (after\npre-termination).  You can specify any length of time, e.g., a week or\neven longer.  The `--signal` flag is used to request that the batch\nsystem sends user-defined signal USR1 to the batch shell (where the\njob is running) `sig_time` seconds (e.g., 300) before the job hits the\nwall limit.  This time should match the checkpoint overhead of your\njob.\n\nUpon receiving the signal USR1 from the batch system (300 seconds before\nthe job hits the wall limit), the `requeue_job` executes the following",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/development/checkpoint-restart/dmtcp/#perlmutter",
            "content": "functions. As a result the single script submission to Slurm can manage the entire process.\n\n1. The Slurm comment field is used to set and track the maximum total compute time\n   which can be requested by the first job and all requeued jobs. If\n   job time expires and its task is still running, a checkpoint is created, the job\n   is requeued, and the comment field updated to reflect how much time remains.\n\n2. As the simplest example, the job is not automatically checkpointed with a timed schedule, but only does so\n   when receiving the system signal from Slurm that it is about to be ended.\n   If scheduled checkpoints are still desired, return the `-i` flag to the `start_coordinator` command. \n\n3. There is only one job id, and one standard output/error file\n   associated with multiple requeued jobs. The Slurm `sacct` command accepts a\n   `--duplicates` flag which can be used to display more complete information about requeued\n   jobs.\n\nThese features are enabled with the following additional `sbatch` flags\nand a bash function `requeue_job`, which traps the signal (USR1) sent\nfrom Slurm:\n\n#SBATCH --comment=12:00:00         #maximum time available to job and all requeued jobs\n#SBATCH --signal=B:USR1@60\n#SBATCH --requeue                  #specify job is requeueable\n#SBATCH --open-mode=append         #to append standard out/err of the requeued job  \n                                   #to that of the previously terminated job\n```\n  \n```shell\n#requeueing the job if remaining time >0",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/development/checkpoint-restart/mana/#perlmutter-cpu",
            "content": "anywhere between the 2 hours guaranteed by the `preempt` QOS and time limit (`-t`),\ncheckpointing once every hour (`-i 3600`).\nIn the C/R job scripts, in addition to loading the mana module, \nthe `nersc_cr` module is loaded as well, which provides a set of bash \nfunctions to manage C/R jobs, e.g., `restart_count`, `requeue_job`,\n`func_trap`, `ckpt_mana`, etc.,  that are used in the job script. \n\nWhat's new in this script is that \n\n1. It can automatically track the remaining walltime, and resubmit\n   itself until the job completes or the accumulated run time reaches\n   the desired walltime (48 hours in this example).   \n\n2. Optionally, each job checkpoints one more time 300 seconds before the\n   job hits the allocated time limit.   \n\n3. There is only one job ID, and one standard output/error file\n   associated with multiple shorter jobs.\n\nThese features are enabled with the following additional sbatch flags\nfrom the batch system:\n\n```slurm\n#SBATCH --comment=48:00:00         #comment for the job\n#SBATCH --signal=B:USR1@<sig_time> \n#SBATCH --requeue                  #specify job is requeueable\n#SBATCH --open-mode=append         #to append standard out/err of the requeued job  \n                                   #to that of the previously terminated job\n```\n  \n```shell\n#requeueing the job if remaining time >0\nckpt_command=ckpt_mana",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/development/checkpoint-restart/dmtcp/#perlmutter",
            "content": "#### Perlmutter \n\n??? example \"`perlmutter-cpu-dmtcp-vt.sh`: a sample job script which runs the payload task while DMTCP operates automatically\"  \n\n    ```slurm\n    --8<-- \"docs/development/checkpoint-restart/dmtcp/examples/perlmutter-cpu-dmtcp-vt.sh\"\n    ```\n\nThis script uses the same basic concepts as the manual start and restart scripts above,\nbut adds automation to reduce the manual actions needed to operate checkpoint-restart\nfunctions. As a result the single script submission to Slurm can manage the entire process.\n\n1. The Slurm comment field is used to set and track the maximum total compute time\n   which can be requested by the first job and all requeued jobs. If\n   job time expires and its task is still running, a checkpoint is created, the job\n   is requeued, and the comment field updated to reflect how much time remains.\n\n2. As the simplest example, the job is not automatically checkpointed with a timed schedule, but only does so\n   when receiving the system signal from Slurm that it is about to be ended.\n   If scheduled checkpoints are still desired, return the `-i` flag to the `start_coordinator` command. \n\n3. There is only one job id, and one standard output/error file\n   associated with multiple requeued jobs. The Slurm `sacct` command accepts a\n   `--duplicates` flag which can be used to display more complete information about requeued\n   jobs.\n\nThese features are enabled with the following additional `sbatch` flags\nand a bash function `requeue_job`, which traps the signal (USR1) sent\nfrom Slurm:",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/development/checkpoint-restart/mana/#perlmutter-cpu",
            "content": "where the `--comment` sbatch flag is used to specify the desired\nwalltime and to track the remaining walltime for the job (after\npre-termination).  You can specify any length of time, e.g., a week or\neven longer.  The `--signal` flag is used to request that the batch\nsystem sends user-defined signal USR1 to the batch shell (where the\njob is running) `sig_time` seconds (e.g., 300) before the job hits the\nwall limit.  This time should match the checkpoint overhead of your\njob.\n\nUpon receiving the signal USR1 from the batch system (300 seconds before\nthe job hits the wall limit), the `requeue_job` executes the following\n`reuque_job` command line in the job script):\n\n```shell\nmana_status --checkpoint     #checkpoint the job if ckpt_command=ckpt_mana  \nscontrol requeue $SLURM_JOB_ID #requeue the job \n```\n\nIf your job completes before the job hits the wall limit, then the\nbatch system will not send the USR1 signal, and the two commands above\nwill not be executed (no additional checkpointing and no more requeued\njob).  The job will exit normally.\n\nthe C/R job scripts, refer to the script `cr_functions.sh` provided by\nthe `nersc_cr` module.  (type `module show nersc_cr` to see where the\nscript resides).  You may consider making a local copy of this script,\nand modifying it for your use case.\n\nTo **run** the job, simply submit the job script, \n\n```shell\nsbatch run_cr.slurm\n```\n\n!!! note",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/development/build-tools/cmake/#build-flag-recommendations",
            "content": "```cmake\ninclude(CheckCCompilerFlag)\ninclude(CheckCXXCompilerFlag)\n\n# create interface target with compiler flags\nadd_library(foo-compile-options INTERFACE)\n\n#----------------------------------------------------------------------------------------#\n# macro that checks if flag if supported for C, if so add to foo-compile-options\n#----------------------------------------------------------------------------------------#\nmacro(ADD_C_FLAG_IF_AVAIL FLAG)\n    if(NOT \"${FLAG}\" STREQUAL \"\")\n        # create a variable for checking the flag if supported, e.g.:\n        #   -fp-model=precise --> c_fp_model_precise\n        string(REGEX REPLACE \"^-\" \"c_\" FLAG_NAME \"${FLAG}\")\n        string(REPLACE \"-\" \"_\" FLAG_NAME \"${FLAG_NAME}\")\n        string(REPLACE \" \" \"_\" FLAG_NAME \"${FLAG_NAME}\")\n        string(REPLACE \"=\" \"_\" FLAG_NAME \"${FLAG_NAME}\")\n\n        check_c_compiler_flag(\"${FLAG}\" ${FLAG_NAME})\n            target_compile_options(foo-compile-options INTERFACE\n                $<$<COMPILE_LANGUAGE:C>:${FLAG}>)\n        endif()\n    endif()\nendmacro()\n\n#----------------------------------------------------------------------------------------#\n# macro that checks if flag if supported for C++, if so add to foo-compile-options\n#----------------------------------------------------------------------------------------#\nmacro(ADD_CXX_FLAG_IF_AVAIL FLAG)\n    if(NOT \"${FLAG}\" STREQUAL \"\")\n        # create a variable for checking the flag if supported, e.g.:",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/applications/vasp/examples/long-time.sh",
            "content": "##                 overrun, or shared-overrun.\n## '--comment=XX'  is the total time that SUM of restarts can run (can be VERY LARGE).\n## '--time=XX'     is the maximum time that individual restart can run. This MUST fit\n##                 inside the queue limit for the QoS that you want to use\n##                 (see https://docs.nersc.gov/jobs/policy/ for details).\n## '--time-min=XX' is the minimum time that job can run before being preempted (using this\n##                 option can make it easier for Slurm to fit into queue, possibly allowing\n##                 job to start sooner). Omit this parameter unless running in either\n##                 --qos=preempt or --qos=debug_preempt.\n##                 (set this large enough to have enough time to write checkpoint file(s)\n##                  before time limit is reached; 60 seconds is usually enough).\n## '--requeue'     specifies job is elegible for requeue in case of preemption.\n## '--open-mode=append' appends contents of to the end of standard output and standard\n##                 error files with successive requeues.\n\n# Remove STOPCAR file so job isn't blocked\nrm STOPCAR\nfi\n\n# Select VASP module of choice\nmodule load vasp/5.4.4-cpu\n\n# srun must execute in background and catch signal on wait command\n# so ampersand ('&') is REQUIRED here\nsrun -n 128 -c 2 --cpu_bind=cores vasp_std &\n\n# Put any commands that need to run to continue the next job (fragment) here\nckpt_vasp() {",
            "is_markdown": false
        }
    ],
    "5": [
        {
            "url": "https://docs.nersc.gov/getting-started/#high-performance-storage-system-hpss-archival-storage",
            "content": "### High Performance Storage System (HPSS) Archival Storage\n\nThe High Performance Storage System (HPSS) is a modern, flexible,\nperformance-oriented mass storage system. It has been used at NERSC\nfor archival storage since 1998. HPSS is intended for long term\nstorage of data that is not frequently accessed.\n\n* [Detailed HPSS usage information](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/filesystems/archive/)",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/help/acronyms/#acronyms",
            "content": "| CMP   | Chip Multicore Processor |\n| CPE   | Cray Programming Environment |\n| CPU   | Central Processing Unit |\n| CRT   | Computational Research and Theory Facility (aka Wang Hall) [map](https://goo.gl/maps/mJYcYDE1rLFQFBsv6) |\n| DDR   | Double Data Rate |\n| DTN   | Data Transfer Node |\n| DVS   | Data Virtualization Service |\n| ERCAP | Energy Research Computer Allocations Program |\n| JGI   | [Joint Genome Institute](https://jgi.doe.gov) |\n| GPC   | (Nvidia GPU's) GPU Processing Cluster |\n| GPFS  | (IBM's) General Purpose File System |\n| GPU   | Graphical Processing Unit |\n| HBM   | High Bandwidth Memory |\n| HPC   | High Performance Computing |\n| HPSS  | High Performance Storage System |\n| HT (also HTT) | Hyperthreading |\n| IP    | [Internet Protocol](https://tools.ietf.org/html/rfc791) |\n| KNL   | Knights Landing |\n| LDAP  | [Lightweight Directory Access Protocol](https://tools.ietf.org/html/rfc4511) |\n| LNET  | Lustre network router |\n| MCDRAM | Multi-Channel DRAM |\n| MDS   | Metadata Server, a component of the Lustre that manages file operation, e.g., create new file, write to shared file |\n| MFA   | Multi-Factor Authentication |",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/help/acronyms/#acronyms",
            "content": "# Acronyms\n\n| Acronym | Term  |\n|:--------|:------|\n| ALCC  | ASCR Leadership Computing Challenge |\n| ASCR  | [Advanced Scientific Computing Research (office of)](https://science.osti.gov/ascr) |\n| ASIC  | Application-Specific Integrated Circuit |\n| CDT   | Cray Developer Toolkit |\n| CFS   | [Community File System](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/filesystems/community/) |\n| CMP   | Chip Multicore Processor |\n| CPE   | Cray Programming Environment |\n| CPU   | Central Processing Unit |\n| CRT   | Computational Research and Theory Facility (aka Wang Hall) [map](https://goo.gl/maps/mJYcYDE1rLFQFBsv6) |\n| DDR   | Double Data Rate |\n| DTN   | Data Transfer Node |\n| DVS   | Data Virtualization Service |\n| ERCAP | Energy Research Computer Allocations Program |\n| JGI   | [Joint Genome Institute](https://jgi.doe.gov) |\n| GPC   | (Nvidia GPU's) GPU Processing Cluster |\n| GPFS  | (IBM's) General Purpose File System |\n| GPU   | Graphical Processing Unit |\n| HBM   | High Bandwidth Memory |\n| HPC   | High Performance Computing |\n| HPSS  | High Performance Storage System |\n| HT (also HTT) | Hyperthreading |",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/help/acronyms/#acronyms",
            "content": "| JGI   | [Joint Genome Institute](https://jgi.doe.gov) |\n| GPC   | (Nvidia GPU's) GPU Processing Cluster |\n| GPFS  | (IBM's) General Purpose File System |\n| GPU   | Graphical Processing Unit |\n| HBM   | High Bandwidth Memory |\n| HPC   | High Performance Computing |\n| HPSS  | High Performance Storage System |\n| HT (also HTT) | Hyperthreading |\n| IP    | [Internet Protocol](https://tools.ietf.org/html/rfc791) |\n| KNL   | Knights Landing |\n| LDAP  | [Lightweight Directory Access Protocol](https://tools.ietf.org/html/rfc4511) |\n| LNET  | Lustre network router |\n| MCDRAM | Multi-Channel DRAM |\n| MDS   | Metadata Server, a component of the Lustre that manages file operation, e.g., create new file, write to shared file |\n| MFA   | Multi-Factor Authentication |\n| MKL   | (Intel) [Math Kernel Library](https://software.intel.com/en-us/mkl) |\n| MOM   | Machine Oriented Mini-server |\n| MOTD  | [Message of the Day](https://www.nersc.gov/live-status/motd/) |\n| MPI   | [Message Passing Interface](https://www.mpi-forum.org) |\n| MPMD  | Multiple Programs, Multiple Data |\n| MPP   | Massively Parallel Processor |\n| MTBF  | Mean Time Between Failures |",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/filesystems/#archivehttpsdocsnerscgovglobalcfscdirsnstaffchatbotproductioncodedatanerscdocdocsfilesystemsarchive-hpss",
            "content": "#### [Archive](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/filesystems/archive/) (HPSS)\n\nA high capacity tape archive intended for long term storage of\ninactive and important data. Accessible from all systems at\nNERSC. Space quotas are allocation dependent.\n\nThe High Performance Storage System (HPSS) is a modern, flexible,\nperformance-oriented mass storage system. It has been used at NERSC\nfor archival storage since 1998. HPSS is intended for long term\nstorage of data that is not frequently accessed.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/filesystems/archive/#introduction",
            "content": "## Introduction\n\nThe High Performance Storage System (HPSS) is a modern, flexible,\nperformance-oriented mass storage system. It has been used at NERSC\nfor archival storage since 1998. HPSS is intended for long term\nstorage of data that is not frequently accessed.\n\nHPSS is Hierarchical Storage Management (HSM) software developed by a\ncollaboration of DOE labs, of which NERSC is a participant, and\nIBM. The HPSS system is a tape system that uses HSM software to ingest\ndata onto a high performance disk cache and automatically migrate it\nto a very large enterprise tape subsystem for long-term retention. The\ndisk cache in HPSS is designed to retain many days worth of new data\nand the tape subsystem is designed to provide the most cost-effective\nlong-term scalable data storage available.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/filesystems/#local-storage",
            "content": "### Local storage\n\nThe following file systems provide high I/O performance, but often\ndon't preserve data across different jobs, so they are meant to be\nused as scratch space, and data produced must be staged out at the\nend of the computation.\n\nAccess is always per-user, since these file systems only\naccessible within the same SLURM job (XFS and in-RAM file systems),\nsince SLURM purges the content afterwards.\n\n\n#### Temporary per-node Shifter file system\n\nShifter users can access a fast, per-node [xfs file\nsystem](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/development/containers/shifter/how-to-use/#temporary-xfs-files-for-optimizing-io)\nto improve I/O.\n\n\n#### Local temporary file system\n\nCompute nodes have a small amount of\n[temporary local storage](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/performance/io/dev-shm/)\nthat can be used to improve I/O.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/policies/data-policy/policy/#archival-storage-hpss",
            "content": "### Archival Storage (HPSS)\n\n[HPSS](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/filesystems/archive/) provides long-term archival storage\nto users at the facility. The main focus of the system is data\nstewardship and preservation, but it also supports some data sharing\nneeds.",
            "is_markdown": true
        }
    ],
    "6": [
        {
            "url": "https://docs.nersc.gov/environment/lmod/#finding-modules",
            "content": "cray-fftw: cray-fftw/3.3.8.9\n\n  cray-hdf5: cray-hdf5/1.12.0.3\n\n  ...\n``` \n\nThe `module spider` can report all versions of the software. For instance if we want to see all\ngcc compilers we can run the following. In this example we have three versions of gcc (**8.1.0**,\n\n```console\nelvis@perlmutter> module spider gcc\n\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  gcc:\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n     Versions:\n        gcc/8.1.0\n        gcc/9.3.0\n        gcc/10.2.0\n\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  For detailed information about a specific \"gcc\" package (including how to load the modules) use the module's full name.\n  Note that names that have a trailing (E) are extensions provided by other modules.\n  For example:\n\n     $ module spider gcc/10.2.0\n```\n\nLmod recently introduced a new command called `module overview` which displays each module in short form with a \nnumber of module versions.  Shown below is a preview of `module overview`\n\n```console\nelvis@perlmutter> module overview\n\n------------------------------------------------------------------------------------------------------------------ /opt/cray/pe/lmod/lmod/modulefiles/Core -------------------------------------------------------------------------------------------------------------------\nlmod (1)   settarg (1)",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/environment/lmod/#finding-modules",
            "content": "------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  gcc:\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n     Versions:\n        gcc/8.1.0\n        gcc/9.3.0\n        gcc/10.2.0\n\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  For detailed information about a specific \"gcc\" package (including how to load the modules) use the module's full name.\n  Note that names that have a trailing (E) are extensions provided by other modules.\n  For example:\n\n     $ module spider gcc/10.2.0\n```\n\nLmod recently introduced a new command called `module overview` which displays each module in short form with a \nnumber of module versions.  Shown below is a preview of `module overview`\n\n```console\nelvis@perlmutter> module overview\n\n------------------------------------------------------------------------------------------------------------------ /opt/cray/pe/lmod/lmod/modulefiles/Core -------------------------------------------------------------------------------------------------------------------\nlmod (1)   settarg (1)\n\naocc (1)   cpe-cuda  (2)   cray-cti     (4)   cray-libpals (2)   cray-pals   (2)   cray-R    (1)   craypkg-gen (2)   gdb4hpc (2)   nvidia         (5)   PrgEnv-aocc (1)   PrgEnv-nvhpc   (1)   valgrind4hpc (2)",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/development/compilers/base/#gnu",
            "content": "### GNU\n\nThe GCC compiler suite is available via the `PrgEnv-gnu` module,\nwhich will load the `gcc` module for the GNU base compilers.\nThe base compilers in this suite are:\n\n * C: `gcc`\n * C++: `g++`\n * Fortran: `gfortran`\n\nSee the [full documentation of the GCC\ncompilers](https://gcc.gnu.org/onlinedocs). Additionally, compiler\ndocumentation is provided through `man` pages (e.g., `man g++`) and\nthrough the `--help` flag to each compiler (e.g., `gfortran --help`).",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/environment/lmod/#seeing-defaults-for-modulefiles",
            "content": "### Seeing Defaults for Modulefiles\n\nThe `module -d avail` command will report the default for every module. Lmod will load the \ndefault module if you don't specify the full version (i.e., `module load gcc`) since there is \nonly one default for every module name. \n\nFor example, to list the default for `gcc`:\n\n```console\nelvis@perlmutter> module -d avail gcc\n\n---------------------------------------------------------------------------- Cray Core Modules -----------------------------------------------------------------------------\n   gcc/10.2.0 (L)\n\n  Where:\n   L:  Module is loaded\n\nUse \"module spider\" to find all possible modules and extensions.\nUse \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\".\n```",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/environment/lmod/#finding-modules",
            "content": "------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  For detailed information about a specific \"gcc\" package (including how to load the modules) use the module's full name.\n  Note that names that have a trailing (E) are extensions provided by other modules.\n  For example:\n\n     $ module spider gcc/10.2.0\n```\n\nLmod recently introduced a new command called `module overview` which displays each module in short form with a \nnumber of module versions.  Shown below is a preview of `module overview`\n\n```console\nelvis@perlmutter> module overview\n\n------------------------------------------------------------------------------------------------------------------ /opt/cray/pe/lmod/lmod/modulefiles/Core -------------------------------------------------------------------------------------------------------------------\nlmod (1)   settarg (1)\n\naocc (1)   cpe-cuda  (2)   cray-cti     (4)   cray-libpals (2)   cray-pals   (2)   cray-R    (1)   craypkg-gen (2)   gdb4hpc (2)   nvidia         (5)   PrgEnv-aocc (1)   PrgEnv-nvhpc   (1)   valgrind4hpc (2)\ncce  (2)   cray-ccdb (1)   cray-dyninst (1)   cray-mrnet   (1)   cray-python (1)   craype    (1)   gcc         (4)   nvhpc   (5)   perftools-base (2)   PrgEnv-gnu  (1)   sanitizers4hpc (2)",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/environment/lmod/#finding-modules",
            "content": "## Finding Modules\n\nLmod provides several commands to help you find modules including `module avail`, `module spider` and even\n`module overview`. We will discuss a few of these commands here. \n\nTo see list of available modules, you can run \n\n```shell\nmodule avail\n```\n\nThe output of this will be quite long depending on the number of modulefiles, for instance if you want to see all `gcc` modules \nyou can run \n\n```shell\nmodule avail gcc \n```\n\nThe `module spider` command reports all modules in your system in `MODULEPATH` along with all module trees in the hierarchical \nsystem. Note that `module avail` doesn't show modules from all trees in the hierarchical system. If you want to know **all** \navailable software on the system, please use `module spider`. \n\nThe output will be a list of software entries with corresponding versions:\n\n```console\nelvis@perlmutter> module spider\n\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nThe following is a list of the modules and extensions currently available:\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  aocc: aocc/2.2.0.1\n\n  atp: atp/3.13.1\n\n  cce: cce/11.0.4\n\n  clingo: clingo/git_20210514\n\n  cmake: cmake/3.18.4\n\n  cpe: cpe/21.04\n\n  cray-ccdb: cray-ccdb/4.11.1\n\n  cray-cti: cray-cti/2.13.6",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/development/build-tools/spack/#loading-spack-packages",
            "content": "```\n    \nLet's assume you want to use PAPI. First, you need to determine which Spack package you want to load.\nWe can run the following to show the installed versions of PAPI.\n\n```\nspack find papi\n```\n\nThis will output a list of installed packages as shown:\n\n```shell\n==> Installed packages\n-- linux-sles15-zen3 / gcc@11.2.0 -------------------------------\npapi@6.0.0.1\n==> 1 installed package\n```\n\nLet's load PAPI into our user environment.\nTo specify the version use `@` and to specify the compiler use `%`. These symbols can also be combined\nas is the case with loading GCC version 11.2.0.\nTake note that Spack will load the dependencies\nin your user environment which is the default behavior.\n\n```shell\nelvis@perlmutter> spack load papi%gcc@11.2.0\n==> 13 loaded packages\n-- linux-sles15-zen3 / gcc@11.2.0 -------------------------------\nberkeley-db@18.1.40  kbproto@1.0.7  libuv@1.44.1  ncurses@6.1   pkg-config@0.29.2  util-linux-uuid@2.36.2  xproto@7.0.31\ninputproto@2.3.2     libmd@1.0.4    lzo@2.10      papi@6.0.0.1  rhash@1.4.2        xextproto@7.3.0\n```",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/development/build-tools/spack/#unloading-spack-packages-from-your-environment",
            "content": "==> 17 loaded packages\n-- linux-sles15-zen3 / gcc@11.2.0 -------------------------------\nbzip2@1.0.6   curl@7.66.0  libarchive@3.6.2  libiconv@1.17  libuv@1.44.1  lzo@2.10        ncurses@6.1  xz@5.4.1     zstd@1.5.5\n\nelvis@perlmutter> which cmake\n/global/common/software/spackecp/perlmutter/e4s-23.05/89639/spack/opt/spack/linux-sles15-zen3/gcc-11.2.0/cmake-3.26.3-j6xprhqvjwkyngr534kqdxnp5uhwf6j4/bin/cmake\n\nelvis@perlmutter> cmake --version\ncmake version 3.26.3\n\nCMake suite maintained and supported by Kitware (kitware.com/cmake).\n```\n\n\n```shell\nelvis@perlmutter> spack unload cmake\nelvis@perlmutter> which cmake\n/usr/bin/cmake\n\nelvis@login40> cmake --version\ncmake version 3.20.4\n\nCMake suite maintained and supported by Kitware (kitware.com/cmake).\n\nelvis@perlmutter> spack load --list\n==> 16 loaded packages\n-- linux-sles15-zen3 / gcc@11.2.0 -------------------------------",
            "is_markdown": true
        }
    ],
    "7": [
        {
            "url": "https://docs.nersc.gov/connect/mfa/#sshproxy-command-line-options",
            "content": "##### sshproxy Command-line Options\n\n`sshproxy.sh` has several command-line options to override its\ndefault behavior. You can run `sshproxy.sh -h` to get a help message.\n\n```console\n$ ./sshproxy.sh -h\nUsage: sshproxy.sh [-u <user>] [-o <filename>] [-s <scope>] [-c <account>] [-p] [-a] [-x <proxy-URL>] [-U <server URL>] [-v] [-h]\n         -u <user>\tSpecify remote (NERSC) username\n                        (default: <your_login_name>)\n         -o <filename>  Specify pathname for private key\n                        (default: <your_home_directory>/.ssh/nersc)\n         -s <scope>     Specify scope (default: 'default')\n         -p             Get keys in PuTTY compatible (ppk) format\n         -a             Add key to ssh-agent (with expiration)\n         -c <account>   Specify a collaboration account (no default)\n         -x <URL>       Specify alternate URL for sshproxy server\n                        (format: <protocol>://<host>[:port], see curl manpage\n                        section on --proxy for details)\n         -U <URL>       Specify alternate URL for sshproxy server\n                        (generally only used for testing purposes)\n         -v             Print version number and exit\n         -h             Print this usage message and exit\n```\n\nIf your NERSC username is not the same as your local username, you\ncan specify your NERSC username with the `-u` option:\n\n```\n./sshproxy.sh -u myusername\n```",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/connect/mfa/#sshproxy",
            "content": "### sshproxy\n\nNERSC has developed a service, called sshproxy, that allows you to\nuse MFA to get an ssh key that is valid for a limited time (24 hours\nby default). sshproxy provides a type of single-sign-on capability\nfor ssh to NERSC systems. Once you have obtained a key, you can use\nit to ssh to NERSC systems without further authentication until the\nkey expires.\n\nThe sshproxy service uses a RESTful API for requesting keys. NERSC\nprovides a bash client script that you can use from the command\nline on a Unix-like computer.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/connect/mfa/#using-sshproxy",
            "content": "##### Using sshproxy\n\nThe sshproxy client, without any arguments, will use your local\nusername, and obtain an ssh key with the default lifetime (24 hours).\nThe private and public key will have the names `nersc` and\n`nersc-cert.pub`, and will be stored in your `~/.ssh directory`.\n\nRun the `sshproxy.sh` script from where you installed it. The script\nwill prompt you to enter your password and OTP, in the same manner as\nyou would do to ssh to a NERSC system with MFA:\n\n```console\n$ ./sshproxy.sh -u <nersc_username>\nEnter your password+OTP:\n```\n\nEnter your NERSC password immediately followed by OTP as a single\nstring, as before. Upon successfully authenticating, the client\nwill install an ssh key and display a message showing the path to\nthe key pair installed on your local computer and the expiration\ndate and time for the keys. By default, the name of the files will\nbe `~/.ssh/nersc` and `~/.ssh/nersc-cert.pub` (you can change the\nname with a command-line argument).\n\n!!! note \n    A quote character in your password could cause `sshproxy.sh` \n    to fail. This is a limitation of the `sshproxy.sh` script - you\n    can either change your password to not contain quotes, or \n    login without using `sshproxy.sh` by entering your password \n    and OTP each time you login  \n\n```console\n$ ./sshproxy.sh -u elvis\nEnter your password+OTP:\nSuccessfully obtained ssh key /Users/elvis/.ssh/nersc",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/connect/mfa/#sshproxy-command-line-options",
            "content": "##### sshproxy Command-line Options\n\n`sshproxy.exe` has several command-line options to override its\ndefault behavior. You can run `sshproxy.exe -h` to get a help\nmessage:\n\n```\nC:\\Users\\myaccount\\Documents>sshproxy.exe -h\nNAME:\n   sshproxy - sshproxy grabs keys from NERSC\n\nUSAGE:\n   sshproxy.exe [global options] command [command options] [arguments...]\n\n...\n\nCOMMANDS:\n     help, h  Shows a list of commands or help for one command\n\nGLOBAL OPTIONS:\n   --debug, -d               Debug logging\n   --user value, -u value    Specify remote (NERSC) username\n   --output value, -o value  Specify pathname for private key (default: \"nersckey.ppk\")\n   --scope value, -s value   key scope (default: \"default\")\n   --collab value, -c value  Specify a collaboration account\n   --server value, -U value  server to grab keys from (default: \"sshproxy.nersc.gov\")\n   --help, -h                show help\n   --version, -v             print the version\n```\n\nIf you would like to have a different name for the PPK file, you\ncan use the `-o` option to specify the output filename:\n\n```\nC:\\Users\\myaccount\\Documents>sshproxy.exe -u myusername -o mynersc.ppk\n```",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/connect/mfa/#installing-the-client",
            "content": "##### Installing the Client\n\nYou can download the bash client `sshproxy.sh` via scp:\n\n```\nscp myusername@dtn01.nersc.gov:/global/cfs/cdirs/mfa/NERSC-MFA/sshproxy.sh .\n```\n\nwhere `myusername` is your NERSC login ID. The above command uses a \ndata transfer node (dtn01), but you can use any machine which you can\naccess that can access the Community file system.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/connect/mfa/#sshproxy-command-line-options",
            "content": "(default: <your_home_directory>/.ssh/nersc)\n         -s <scope>     Specify scope (default: 'default')\n         -p             Get keys in PuTTY compatible (ppk) format\n         -a             Add key to ssh-agent (with expiration)\n         -c <account>   Specify a collaboration account (no default)\n         -x <URL>       Specify alternate URL for sshproxy server\n                        (format: <protocol>://<host>[:port], see curl manpage\n                        section on --proxy for details)\n         -U <URL>       Specify alternate URL for sshproxy server\n                        (generally only used for testing purposes)\n         -v             Print version number and exit\n         -h             Print this usage message and exit\n```\n\nIf your NERSC username is not the same as your local username, you\ncan specify your NERSC username with the `-u` option:\n\n```\n./sshproxy.sh -u myusername\n```\n\nyou can use the `-o` option to specify the output filename:\n\n```\n./sshproxy.sh -o mynersc\n```\n\nNote the `-a` option can be used to automatically add the new key\nto your `ssh-agent`.  It will also be set with an expiration that\nmatches the keys expiration so that ssh does not try to use the key\nafter it has expired.\n\nIf your computer has an old version of ssh (e.g., OpenSSH_7.2), you\nmay have to use the `-a` flag. Otherwise, ssh and scp commands will\nrequire additional flags to work as in the example cases shown\nbelow. To see the version info, run the command, `ssh -V`.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/connect/mfa/#installing-the-client",
            "content": "##### Installing the Client\n\nUse your favorite file transfer tool and download the following\nWindows executable:\n\n```\n/global/cfs/cdirs/mfa/NERSC-MFA/sshproxy.exe\n```",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/connect/nx/#configure-a-nersc-connection",
            "content": "Check out this video (note the Client version is old but the general settings\n    remain the same) which shows you how to install NoMachine from scratch and\n    configure using `sshproxy`.\n    \n    <iframe width=\"560\" height=\"315\"\n    src=\"https://www.youtube.com/embed/sdlD6CUSE9o\" frameborder=\"0\"\n    allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>\n    1. Not shown in video but required: To enable `sshproxy` in NoMachine, you will need to edit one of the NoMachine\n       config files on your local machine. **You must edit this file while NoMachine is\n       closed/not running**. First exit the NoMachine\n       program and then edit `$HOME/.nx/config/player.cfg` and\n       change the following key from `library` to `native`: `<option key=\"SSH client mode\" value=\"native\" />`\n    1. [Install sshproxy on your laptop and generate an SSH key](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/connect/mfa/#sshproxy).\n       Note that you must do this once every day to generate a new key.\n    1. Open the NoMachine client and click on the green Add icon in the upper left corner\n    1. Under Address > Host must be `nxcloud01.nersc.gov`\n    1. Under Address > Protocol set it to `ssh`. This should automatically change the Port to 22.\n    1. Under Configuration > Authentication, select \"use key-based authentication with a key you provide\"\n    1. Click Modify. Fill in the path to the ssh key you generated (usually",
            "is_markdown": true
        }
    ],
    "8": [
        {
            "url": "https://docs.nersc.gov/getting-started/#email",
            "content": "## Email\n\nWhen you get a NERSC user account, an email alias is created for you\nof the form `<username>@nersc.gov` that will forward email to your\ndelivery address (set in [Iris](https://iris.nersc.gov)).\n\n!!! note \"NERSC users email list\"\n\tAs a NERSC user you will be subscribed to the NERSC users\n\temail list. Users cannot unsubscribe from this list due to the\n\timporant nature of the information.\n\nNERSC provides several other email lists, including one with\nadditional system status details. [NERSC Email\nlists](https://www.nersc.gov/news-publications/announcements/email-lists/).",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/accounts/#if-you-have-an-existing-nersc-user-account-and-want-to-join-another-project",
            "content": "### If you have an existing NERSC user account and want to join another project\n\nIf you are an existing NERSC user and need to be added to a new/another project,\nthe project PI or one of their proxies can add you. You must know the name of the PI\nand the name of the Project/repository. You can then either:\n\n 1. Contact the PI and request that they add your account to their project \n\nOR\n\n 2. Go to the [NERSC New Account Request](https://iris.nersc.gov/add-user) page and \nclick on the \"I have a current NERSC account\" button, enter your \"Existing Username\"\nand then search for the project by either entering the PI's name or the Project Name \nin the text box. The PI or one of their Proxies will need to approve your request and\nwhen that is done, you will receive an email notification",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/iris/iris-for-pis/#for-a-user-with-an-active-nersc-account",
            "content": "### For a User with an Active NERSC Account\n\nTo add an existing user who has an active NERSC account:\n\n1.  Select the project from the 'Projects' pull-down menu on the\n    top menu bar.\n2.  Select the 'Roles' tab.\n3.  Click the '+ Add User' button.\n4.  Type the user's name, username or email in the 'Select a user'\n    box.\n5.  Wait until Iris returns a list of the users matching the entered\n    text. Select the user from the list.\n6.  In the dialog box that appears:\n    -   Set the project role for the user:\n\t`user`, `project_resource_manager`, `project_membership_manager`,\n\tor `pi_proxy`.\n\t- NOTE: There can be only one user with the PI role per\n\t  project.\n    -   Set a NERSC hour quota in either the 'Allocated Hours' or\n\t'% of Project's Hours' box. If both are set, the percentage\n\tvalue will be used. Set the HPSS quota for the user in the\n\t'% of HPSS Storage' box.\n    -   Click 'Save Changes.'\n7.  Once you enter the data for the user's account, it will added\n    to your project and the user will be sent an email notification.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/iris/iris-for-users/#navigational-items-at-the-top",
            "content": "submenus by various devices.  Please see the [UI\n    settings](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/iris/#iris_ui_settings) section.\n\n-   The **Iris** icon next to it is clickable and serves\n    as the Home button. This brings you back to the \"Homepage\" which\n\n-   The **search box** allows you to quickly get information that\n    you want about an individual user or a project.\n\n-   Your account name is displayed in the top right corner and is\n    clickable. It brings you to your account's [Profile](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/iris/#profile)\n    page.\n\n-   To logout, click on the hamburger icon, and then the **Logout**\n    button.\n\nmenu bar with several menu tabs. When this bar appears, the first\nvalue on the left is to give you the context for the page displayed.\nFor example, if the page content is about your account itself (that\nis, when the Iris icon is selected), the label will be your full\nname, as shown above. If the content is about a project account,\nit will display the project's name.\n\nThese menu bars are fixed in the page and do not scroll away even\nwhen you scroll down the page.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/iris/iris-for-pis/#navigational-items-at-the-top",
            "content": "submenus by various devices.  Please see the [UI\n    settings](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/iris/#iris_ui_settings) section.\n\n-   The **Iris** icon next to it is clickable and serves\n    as the Home button. This brings you back to the \"Homepage\" which\n\n-   The **search box** allows you to quickly get information that\n    you want about an individual user or a project.\n\n-   Your account name is displayed in the top right corner and is\n    clickable. It brings you to your account's [Profile](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/iris/#profile)\n    page.\n\n-   To logout, click on the hamburger icon, and then the **Logout**\n    button.\n\nmenu bar with several menu tabs. When this bar appears, the first\nvalue on the left is to give you the context for the page displayed.\nFor example, if the page content is about your account itself (that\nis, when the Iris icon is selected), the label will be your full\nname, as shown above. If the content is about a project account,\nit will display the project's name.\n\nThese menu bars are fixed in the page and do not scroll away even\nwhen you scroll down the page.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/accounts/policy/#account-deactivation",
            "content": "## Account Deactivation\n\nThere are several ways that an account could become deactivated.\n\n- **Allocation-year discontinuation:** If a PI does not elect to continue a\nuser in their renewed project in the new allocation year, and that user is not\na member of another active project in the new allocation year, then the user\naccount will be discontinued at the start of the new allocation year.\n- **User-initiated:** A user who wants to close their account can do so in\n[Iris](https://iris.nersc.gov/) or by emailing NERSC account support \nat accounts@nersc.gov with their request to deactivate their account.\n- **PI-initiated:** A PI may remove a user from their project during the\nallocation year. If the user is not a member of another project, their account\nwould no longer be associated with an active project, and the account would\nbecome deactivated.\n- **Policy Agreement expiration:** Periodically, NERSC may update its\n[Appropriate Use Policy](https://www.nersc.gov/users/policies/appropriate-use-of-nersc-resources/)\nand\n[Code of Conduct](https://www.nersc.gov/users/nersc-code-of-conduct/). \nUsers who do not attest to the new policies may have their accounts deactivated\nafter the deadline to accept the new policies passes.\n- **Security deactivation:** If a user violates NERSC security policy or for\nother security-related reasons, NERSC may deactivate a user account.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/applications/wrf/publications/#list-of-publications-using-wrf-by-nersc-users",
            "content": "# List of publications using WRF by NERSC users \n\n[//]: # (comment: use \"lazy numbering\" for a long, changing list - markdown automatically numbers each item)\n[//]: # (comment: it would be nice if we can use google form that a user can input required data, which is then forwarded to a SIG member by email)\n\n1. Chan, M.-Y., Chen X. and Anderson J. L. (2023). The potential benefits of handling clear and cloudy ensemble\n members separately through an efficient bi-Gaussian EnKF. Journal of Advances in Modeling Earth Systems. \n doi: 10.1029/2022MS003357\n\n1. Chen, X., Leung, L. R., Gao, Y., Liu, Y., Wigmosta, M., & Richmond, M. (2018). \nPredictability of Extreme Precipitation in Western U.S. Watersheds Based on Atmospheric River Occurrence,\n Intensity, and Duration. Geophysical Research Letters \n(Vol. 45, Issue 21, pp. 11,693-11,701). https://doi.org/10.1029/2018GL079831\n\n1. Ovchinnikov, M., Fast, J. D., Berg, L. K., Gustafson, W. I., Chen, J., Sakaguchi, K., & Xiao, H. (2022). \n Effects of Horizontal",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/jobs/monitoring/#email-notification",
            "content": "## Email notification\n\nYou can add directives within your job script to notify you when\nyour job starts, finishes, or fails. Using the `--mail-type` \noption, you can select one of `begin`, `end`, or `fail` \n(respectively), or two or more in a comma-separated list (as \nbelow). You should specify the email address to which the \nnotifications should go with the `--mail-user` option.\n\n```\n#SBATCH --mail-type=begin,end,fail\n#SBATCH --mail-user=user@domain.com\n```",
            "is_markdown": true
        }
    ],
    "9": [
        {
            "url": "https://docs.nersc.gov/connect/#perlmutter",
            "content": "#### Perlmutter\n\n```",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/connect/#connecting-to-perlmutter-with-a-collaboration-account",
            "content": "### Connecting to Perlmutter with a Collaboration Account\n\nPlease use the [direct\nlogin](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/accounts/collaboration_accounts/#direct-login)\nfunctionality of [sshproxy](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/connect/mfa/#sshproxy)\nto log into Perlmutter.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/connect/#connect-to-nersc-computational-systems",
            "content": "## Connect to NERSC Computational Systems\n\nPlease make sure you have [established a user account](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/accounts/) and have configured [Multi-Factor Authentication (MFA)](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/connect/mfa/)\nprior to login.\n\nTo access Perlmutter via `ssh` you can do the following:\n\n```\nssh <user>@perlmutter.nersc.gov\n```\n\nor\n\n```\nssh <user>@saul.nersc.gov\n```\n\nIf you have configured [sshproxy](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/connect/mfa/#sshproxy) then you can run the following:\n\n```\nssh -i ~/.ssh/nersc <user>@perlmutter.nersc.gov   # or 'ssh -i ~/.ssh/nersc <user>@saul.nersc.gov'\n```\n\nThis assumes your identity file is in `~/.ssh/nersc`. The sshproxy\nroute will be convenient if you have multiple SSH connections without\nhaving to authenticate every time.\n\nYou can also connect to Perlmutter from a [DTN](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/systems/dtn/#access) and then",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/connect/#connecting-with-ssh",
            "content": "### Connecting with SSH\n\nOn a system with an SSH client installed, one can log into the\nPerlmutter login nodes by running the following command:\n\n```\nssh <username>@perlmutter.nersc.gov   # or 'ssh <username>@saul.nersc.gov'\n```\n\nIf the user has generated a temporary SSH key using\n[`sshproxy`](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/connect/mfa/#sshproxy), then this command will connect the user to one\nof Perlmutter's login nodes. If the user has not generated a temporary SSH key, then\nSSH will challenge the user for their [Iris](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/iris/iris-for-users/)\npassword as well as their [one-time password\n(OTP)](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/connect/mfa/#configuring-and-using-an-mfa-token) before connecting them to a\nlogin node.\n\nUsers are strongly encouraged to use the most up-to-date versions of SSH\nclients that are available.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/connect/mfa/#login-to-nersc-machines",
            "content": "back in. To set the configuration, click on the PuTTY icon on your\nmachine. Let's say that you want to create a configuration for\nconnecting to Perlmutter,\n\n-   Put `myusername@perlmutter.nersc.gov` in the 'Host Name (or IP\n    address)' field.\n\n-   After selecting the 'Connection' category and 'Credentials' from the 'Auth' submenu,\n    click the 'Browse...' button in the 'Private key file for\n    authentication' field to select a PPK file.\n\n    Sessions' field (e.g., perlmutter) and click the 'Save' button to\n    save the configuration.\n\n![Screenshot of PuTTy configuration](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/connect/images/putty-configuration1.png){: height=\"410\" }\n{: align=\"center\" }\n{: align=\"center\" }\n![Screenshot of PuTTy configuration](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/connect/images/putty-configuration3.png){: height=\"410\" }\n{: align=\"center\" }\n\nWhen you want to login to Perlmutter next time, you choose that configuration\nin the 'Saved Sessions' area, click the 'Load' button and then\n'Open'.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/development/languages/python/python-intro/#how-to-run-python-jobs-at-nersc",
            "content": "## How to run Python jobs at NERSC\n\nPlease see our [general overview of running jobs at NERSC](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/jobs/)\n\nYou have many options for running Python at NERSC:\n\n1. Our login nodes (only for very small testing and debugging). Please see our\n[login node policies](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/connect/).\n1. [Jupyter](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/services/jupyter/) for interactive notebooks well-suited\nfor visualization and machine learning tasks.\n1. [CPU compute nodes](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/systems/perlmutter/architecture/#cpu-nodes) or\n[GPU compute nodes](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/systems/perlmutter/architecture/#gpu-nodes) for any\nsubstantial computation (either interactively or via a batch job)",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/connect/nx/#configure-a-nersc-connection",
            "content": "1. Click the yellow Connect button at the top right. Enter your NERSC username.\n    1. Verify the key. Create a new desktop. You should land at the NERSC homescreen.\n    1. You can connect to Perlmutter via the green buttons on the left hand menu.\n       GNOME desktop. If you click `x` without logging out, your session may hang\n       which will prevent you from starting a new session. If you think this has\n       happened to you, please open a ticket at `help.nersc.gov` so we can help\n       you reset your NoMachine access.\n    1. To use this connection in the future, when you start the NoMachine client, just\n       click in the icon you named \"Connection to NERSC\".\n\n??? example \"Authenticate with password and OTC\"\n\n    1. Open the NoMachine client and click on the green Add icon in the upper left corner\n    1. Under Address > Name the connection something like \"Connection to NERSC\"\n    1. Under Address > Protocol set it to `ssh`. This should automatically change the Port to 22.\n    1. Under Configuration > Authentication, select \"Use password authentication\"\n    1. Click the yellow Connect button in the top right corner\n    1. Type your NERSC username and password+OTP. Don't save your password in the connection file; you will need to enter a new OTP every time you log on.\n    1. Verify the key. Create a new desktop You should land at the NERSC homescreen.\n    1. You can connect to Perlmutter via the green buttons on the left hand menu.\n    1. To safely log out, click the `log out` button on the left hand menu of the",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/connect/nx/#configure-a-nersc-connection",
            "content": "and click the back error at the top left.\n    1. Click the yellow Connect button at the top right. Enter your NERSC username.\n    1. Verify the key. Create a new desktop. You should land at the NERSC homescreen.\n    1. You can connect to Perlmutter via the green buttons on the left hand menu.\n       GNOME desktop. If you click `x` without logging out, your session may hang\n       which will prevent you from starting a new session. If you think this has\n       happened to you, please open a ticket at the [NERSC Help Desk](https://help.nersc.gov)\n       so we can help you reset your NoMachine access.\n    1. To use this connection in the future, when you start the NoMachine client, just\n       click in the icon you named \"Connection to NERSC\".\n\n??? example \"Authenticate via sshproxy (Windows)\"\n\n       config files on your local machine. **You must edit this file while NoMachine is\n       closed/not running**. Make sure you can view hidden files and\n       navigate to `C:\\Users\\<yourusername>\\.nx\\config\\player.cfg`. You can edit this file\n       using Notepad++ or other programs that allow you to edit hidden files\n       which cannot be edited by default in Windows.\n       Change the following key from `library` to `native`: `<option key=\"SSH client mode\" value=\"native\" />`\n    1. [Install sshproxy on your laptop and generate an SSH key](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/connect/mfa/#sshproxy).\n       Note that you must do this once every day to generate a new key.",
            "is_markdown": true
        }
    ],
    "10": [
        {
            "url": "https://docs.nersc.gov/connect/#perlmutter",
            "content": "#### Perlmutter\n\n```",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/environment/lmod/#module-hierarchies",
            "content": "...\n\nperlmutter$ module avail aaa\nNo module(s) or extension(s) found!\n...\n```\n\ntruly not available but that `cray-netcdf` exists but can not be loaded directly.\nIt also suggests using `module spider` to see how to load it:\n\n```\nperlmutter$ module load aaa\nLmod has detected the following error:  The following module(s) are unknown: \"aaa\"\n...\nperlmutter$ module load cray-netcdf\nLmod has detected the following error:  These module(s) or extension(s) exist but cannot be loaded as requested: \"cray-netcdf\"\n  Try: \"module spider cray-netcdf\" to see how to load the module(s).\n```\n\nUsing `module spider` gives the available versions, and suggests to use the\n`spider` command on a specific version, which then gives the list of all the\ndependency modules you need to load first (modules on any one of the lines\ngiven depending on your user environment choices such as the compiler version):\n\n!!! tip",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/applications/wrf/wrf/#example-wrf-build-script-for-perlmutter",
            "content": "#module for WRF file I/O\n#order of loading matters!\nmodule load cray-hdf5  #required to load netcdf library\nmodule load cray-netcdf\t\nmodule load cray-parallel-netcdf\n    \nmodule list #check what modules are loaded\n\n#set environmental variables used by WRF build system, \n#using the environmental variables set by the modules\n\n#use classic (CDF1) as default\nexport NETCDF_classic=1 \n#use 64-bit offset format (CDF2) of netcdf files              \n#do not use netcdf4 compression (serial), need hdf5 module\nexport USE_NETCDF4_FEATURES=0         \n\nexport HDF5=$HDF5_DIR\nexport HDF5_LIB=\"$HDF5_DIR/lib\"\nexport HDF5_BIN=\"$HDF5_DIR/bin\"\n\nexport NETCDF=$NETCDF_DIR\nexport NETCDF_BIN=\"$NETCDF_DIR/bin\"\n\n#create PNETCDF environment variable to use the parallel netcdf library\nexport PNETCDF=$PNETCDF_DIR  \n\nexport LD_LIBRARY_PATH=\"/usr/lib64\":${LD_LIBRARY_PATH}\nexport PATH=${NETCDF_BIN}:${HDF5_BIN}:${PATH}\nexport LD_LIBRARY_PATH=${NETCDF_LIB}:${LD_LIBRARY_PATH}\n\n#other special flags\nexport PNETCDF_QUILT=\"0\"  #Quilt output is not stable, better not use it\n\n#check environment variables",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/development/compilers/wrappers/#use-cpe-modules-to-control-versions-of-cray-pe-modules",
            "content": "#### Use `cpe` modules to control versions of Cray PE modules\n\nTo use a non-default CPE (Cray Programming Environment) version on Perlmutter\nwhich includes `craype`, `cray-libsci`, `cray-mpich`, etc. from that specific\nversion, one could issue the following commands first. Below is an\nexample:\n\n```shell\nmodule load cpe/<the-non-default-version>\nexport LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH\n```\n\nThen, compile and run as usual.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/development/languages/python/using-python-perlmutter/#cunumeric",
            "content": "### cuNumeric \n\n[cuNumeric](https://nv-legate.github.io/cunumeric) is a [Legate](https://github.com/nv-legate/legate.core)\nlibrary that aims to provide a distributed and accelerated drop-in replacement\nfor the NumPy API on top of the [Legion](https://legion.stanford.edu/) runtime.\n\nThe following instructions demonstrate how to install cuNumeric using conda for use on a \nsingle Perlmutter GPU node. On a single Perlmutter GPU node: \n\n```shell\n# install cunumeric using conda\nmodule load conda\nconda create -n cunumeric -c nvidia -c conda-forge -c legate cunumeric \nconda activate cunumeric\n# download cunumeric repo with examples\ngit clone https://github.com/nv-legate/cunumeric.git\ncd cunumeric\n# run an example program using 4 GPUS \n# note use of the legate driver to launch the program\nlegate --gpus 4 examples/gemm.py -n 8000\n```\n\nMulti-node installation currently require building legate-core and cuNumeric\nfrom source, see the [nv-legate/quickstart repo](https://github.com/nv-legate/quickstart) for\ndetails.\n\ncuNumeric is currently under active development and should be considered as experimental \nor \"beta\" software. If you have any issues, we recommend opening an issue on the\ncuNumeric github project.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/development/containers/shifter/examples/#star",
            "content": "### STAR\n\nThe [STAR](https://www.star.bnl.gov/) experiment software stack is\ntypically built and run on Scientific Linux.\n\nThere are two ways we can build the STAR image, the first is to\ncompile all the stack components one by one. The other is to install\nthe compiled libraries by copying them into the image. We chose to do\nthe latter in this example.\n\nWe use an SL6.4 docker base image that\nis [publicly](https://hub.docker.com/r/ringo/scientific/tags/)\navailable, install the needed rpms, extract pre-compiled binaries\ntarballs into the image and finally install some software that needed\nto run STAR jobs.\n\n```docker\n# Example Dockerfile to show how to build STAR\n# environment image from binuaries tarballs. Not necessarily\n# the one currently used for STAR docker image build\nFROM ringo/scientific:6.4\nMAINTAINER Mustafa Mustafa <mmustafa@lbl.gov>\n\n# RPMs\nRUN yum -y install libxml2 tcsh libXpm.i686 libc.i686 libXext.i686 \\\n                   libXrender.i686 libstdc++.i686 fontconfig.i686 \\\n                   zlib.i686 libgfortran.i686 libSM.i686 mysql-libs.i686 \\\n                   gcc-c++ gcc-gfortran glibc-devel.i686 xorg-x11-xauth \\",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/applications/wrf/wps/#example-build-script-for-the-jasper-external-library",
            "content": "#one could use the one installed under the Spack module/environment\n#%module load cpu\n#%module load spack\n#%spack load jasper\n#(will be tested)\n\n# WRF/WPS directories, using the WRF-SIG directory as example\nwrfroot=/global/cfs/cdirs/m4232/model/pm/v4.4\n\nexport WRF_DIR=${wrfroot}/WRF\nexport WPS_DIR=${wrfroot}/WPS\n\n#Load the same modules used for WRF, saved in a bash script ---------------------------\nloading_script=\"/global/cfs/cdirs/m4232/scripts/build/load_modules_2023-09_wrfsig.sh\"\nsource ${loading_script}\n\n#directory for the external libraries, here using a global common for WRF-SIG\nexport EXDIR=/global/common/software/m4232/test/LIBRARIES\nmkdir -p ${EXDIR}\n\nlibcodedir=/global/cfs/cdirs/m4232/model/external #where source code of external library is downloaded \n\n#so use the base compiler to build the external library\n\nexport CC=gcc\nexport CXX=g++\nexport FC=gfortran\nexport F77=gfortran\n\necho \"installing JasPer\"\n\ncd ${libcodedir}/jasper-1.900.1\n./configure --prefix=${EXDIR}/grib2\nmake\nmake install\n\n```",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/applications/e4s/spack_environments/#spack-upstreams",
            "content": "```yaml\n  upstreams:\n    perlmutter-e4s-23.08:\n      install_tree: /global/common/software/spackecp/perlmutter/e4s-23.08/default/spack/opt/spack\n```\n\nLet's say you want to install `zlib`. Take note, we install the packages in the\nSpack upstream location since they were previously installed:\n\n```shell\nelvis@perlmutter> spack install --add zlib\n==> Installing gmake-4.4.1-6b72ibntxvoxfjc3vs32fsm3wlxccjgu [1/2]\n==> No binary for gmake-4.4.1-6b72ibntxvoxfjc3vs32fsm3wlxccjgu found: installing from source\n==> No patches needed for gmake\n==> gmake: Executing phase: 'autoreconf'\n==> gmake: Executing phase: 'configure'\n==> gmake: Executing phase: 'build'\n==> gmake: Executing phase: 'install'\n==> gmake: Successfully installed gmake-4.4.1-6b72ibntxvoxfjc3vs32fsm3wlxccjgu\n  Stage: 0.17s.  Autoreconf: 0.00s.  Configure: 14.96s.  Build: 1.29s.  Install: 22.98s.  Post-install: 0.09s.  Total: 39.56s",
            "is_markdown": true
        }
    ],
    "11": [
        {
            "url": "https://docs.nersc.gov/jobs/monitoring/#squeue",
            "content": "```\n\nTo view all running jobs for the current user: \n\n```\nsqueue --me -t RUNNING\n``` \n\nTo view all pending jobs for current user:\n\n```\nsqueue --me -t PENDING\n```\n\nTo view all pending jobs in QOS `shared`:\n\n```\nsqueue -q shared -t PENDING\n```\n\nTo view all running jobs for current user on `shared` qos: \n\n```console\n$ squeue --me -q shared -t RUNNING  \n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n              1000    shared netcdf_r    user1  R    1:16:47      1 nid006504\n```\n\nTo view all jobs for a particular account (project), use `-A <nersc_project>`:\n\n```console\n$ squeue -A <nersc_project>\n          2000 regular_m tokio-ab    admin1 PD       0:00    256 (Priority)\n          2001 regular_m mpi4py-i    admin2 PD       0:00    150 (Priority)\n          2002 regular_m mpi4py-i    admin3 PD       0:00    150 (Priority)\n          2003 regular_m preproce    admin4 PD       0:00      1 (Priority)          \n```\n\nTo view filter jobs, use the `-j` option followed by the job ID. You can specify\nmultiple job IDs separated by commas.\n\n```console",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/jobs/monitoring/#squeue",
            "content": "To view all pending jobs in QOS `shared`:\n\n```\nsqueue -q shared -t PENDING\n```\n\nTo view all running jobs for current user on `shared` qos: \n\n```console\n$ squeue --me -q shared -t RUNNING  \n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n              1000    shared netcdf_r    user1  R    1:16:47      1 nid006504\n```\n\nTo view all jobs for a particular account (project), use `-A <nersc_project>`:\n\n```console\n$ squeue -A <nersc_project>\n          2000 regular_m tokio-ab    admin1 PD       0:00    256 (Priority)\n          2001 regular_m mpi4py-i    admin2 PD       0:00    150 (Priority)\n          2002 regular_m mpi4py-i    admin3 PD       0:00    150 (Priority)\n          2003 regular_m preproce    admin4 PD       0:00      1 (Priority)          \n```\n\nTo view filter jobs, use the `-j` option followed by the job ID. You can specify\nmultiple job IDs separated by commas.\n\n```console\n         JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n          2542 shared_mi wrfpostp    user1 PD       0:00      1 (Dependency)",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/iris/iris-for-users/#jobs",
            "content": "month. To view job data with different start and end dates or for\na specific host (e.g., `perlmutter cpu`, `perlmutter gpu`,\n...), etc., enter the appropriate values for your search, and click\nthe 'Load Jobs' button.\n\nClicking on a job ID link from the displayed result, you will see\na summary about the job such as host; job name; allocation account;\ncharge info; submit, start and end times; the compute nodes used,\netc.\n\n![Iris my jobs job details](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/iris/images/iris_mystuff_jobs_jobdetails.png)\n{: align=\"center\" style=\"border:1px solid black\"}\n\nBelow there is a section titled '**Job Transfers**'. This area\nprovides a record of all deposits and withdrawals made to/from your\nproject's allocation accounts.\n\nBelow it, there is another section titled '**DataCollect\nInfo**'. When the 'More Info' button is clicked, it shows details\nsuch as the working directory, arguments, `srun` parameters, etc.\n\nAt the bottom of the page, there is the '**Display Job Metrics**'\nsection, still in an experimental stage, which, when fully implemented,\nwill display plots of some performance data (e.g, instructions per\ncycle (`ipc_by_step` and `ipc_by_step_and_node`) and memory usage)\nand power usage of the job. Select a metric and then click the 'View\ngraph' button.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/iris/iris-for-users/#jobs-a-namepjobsa",
            "content": "By default, the page lists the CPU jobs run for the last 1 month.\nTo view job data with different start and end dates or for a specific\nhost (e.g., `perlmutter cpu`, `perlmutter gpu`, ...), etc., enter\nthe appropriate values for your search in the boxes at the top, and\nclick the 'Load Jobs' button.\n\nNote that, in order to display Perlmutter GPU jobs, you need to use\nthe GPU account name (e.g., `m1234_g`) in the 'Account name' field.\n\nClicking on a job ID link from the displayed result, you will see\na summary about the job such as host; job name; allocation account;\ncharge info; submit, start and end times; the compute nodes used,\netc.\n\n![Iris projects jobs job details](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/iris/images/iris_projects_jobs_jobdetails.png)\n{: align=\"center\" style=\"border:1px solid black\"}\n\nScrolling down the page, you can find a section titled '**Job\nwithdrawals made to/from your project's allocation accounts.\n\nBelow it, there is a panel titled '**DataCollect Info**'. When the\n'More Info' button is clicked, it shows details for individual job\nsteps (that is, individual `srun` instances), such as the working\ndirectory, arguments, `srun` parameters, etc.\n\nAt the bottom of the page, there is the '**Display Job Metrics**'\nsection, still in an experimental stage, which, when fully implemented,\nwill display plots of some performance data (e.g, instructions per",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/iris/iris-for-users/#jobs",
            "content": "### Jobs\n\nThis page lists jobs that you ran on NERSC machines, along with a\ntime-series plot for the jobs on the top and individual jobs at the\nbottom.\n\n![Iris my jobs](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/iris/images/iris_mystuff_jobs.png)\n{: align=\"center\" style=\"border:1px solid black\"}\n\nBy default, the page lists the jobs you ran during the last one\nmonth. To view job data with different start and end dates or for\na specific host (e.g., `perlmutter cpu`, `perlmutter gpu`,\n...), etc., enter the appropriate values for your search, and click\nthe 'Load Jobs' button.\n\nClicking on a job ID link from the displayed result, you will see\na summary about the job such as host; job name; allocation account;\ncharge info; submit, start and end times; the compute nodes used,\netc.\n\n![Iris my jobs job details](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/iris/images/iris_mystuff_jobs_jobdetails.png)\n{: align=\"center\" style=\"border:1px solid black\"}\n\nBelow there is a section titled '**Job Transfers**'. This area\nprovides a record of all deposits and withdrawals made to/from your\nproject's allocation accounts.\n\nBelow it, there is another section titled '**DataCollect\nInfo**'. When the 'More Info' button is clicked, it shows details",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/development/checkpoint-restart/dmtcp/#perlmutter",
            "content": "3. There is only one job id, and one standard output/error file\n   associated with multiple requeued jobs. The Slurm `sacct` command accepts a\n   `--duplicates` flag which can be used to display more complete information about requeued\n   jobs.\n\nThese features are enabled with the following additional `sbatch` flags\nand a bash function `requeue_job`, which traps the signal (USR1) sent\nfrom Slurm:\n\n#SBATCH --comment=12:00:00         #maximum time available to job and all requeued jobs\n#SBATCH --signal=B:USR1@60\n#SBATCH --requeue                  #specify job is requeueable\n#SBATCH --open-mode=append         #to append standard out/err of the requeued job  \n                                   #to that of the previously terminated job\n```\n  \n```shell\n#requeueing the job if remaining time >0\nrequeue_job func_trap USR1\n\nwait\n```\n\nwhere the `--comment` sbatch flag is used to specify the desired\nwalltime and to track the remaining walltime for the job (after\npre-termination).  You can specify any length of time, e.g., a week or\neven longer.  The `--signal` flag is used to request that the batch\nsystem sends user-defined signal USR1 to the batch shell (where the\njob is running) `sig_time` seconds (e.g., 60) before the job hits the\nwall limit.  i\n\nUpon receiving the signal USR1 from the batch system 60 seconds before\nthe job hits the wall limit, the `requeue_job` executes the following",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/applications/vasp/examples/long-time.sh",
            "content": "##                 overrun, or shared-overrun.\n## '--comment=XX'  is the total time that SUM of restarts can run (can be VERY LARGE).\n## '--time=XX'     is the maximum time that individual restart can run. This MUST fit\n##                 inside the queue limit for the QoS that you want to use\n##                 (see https://docs.nersc.gov/jobs/policy/ for details).\n## '--time-min=XX' is the minimum time that job can run before being preempted (using this\n##                 option can make it easier for Slurm to fit into queue, possibly allowing\n##                 job to start sooner). Omit this parameter unless running in either\n##                 --qos=preempt or --qos=debug_preempt.\n##                 (set this large enough to have enough time to write checkpoint file(s)\n##                  before time limit is reached; 60 seconds is usually enough).\n## '--requeue'     specifies job is elegible for requeue in case of preemption.\n## '--open-mode=append' appends contents of to the end of standard output and standard\n##                 error files with successive requeues.\n\n# Remove STOPCAR file so job isn't blocked\nrm STOPCAR\nfi\n\n# Select VASP module of choice\nmodule load vasp/5.4.4-cpu\n\n# srun must execute in background and catch signal on wait command\n# so ampersand ('&') is REQUIRED here\nsrun -n 128 -c 2 --cpu_bind=cores vasp_std &\n\n# Put any commands that need to run to continue the next job (fragment) here\nckpt_vasp() {",
            "is_markdown": false
        },
        {
            "url": "https://docs.nersc.gov/development/checkpoint-restart/dmtcp/#perlmutter",
            "content": "functions. As a result the single script submission to Slurm can manage the entire process.\n\n1. The Slurm comment field is used to set and track the maximum total compute time\n   which can be requested by the first job and all requeued jobs. If\n   job time expires and its task is still running, a checkpoint is created, the job\n   is requeued, and the comment field updated to reflect how much time remains.\n\n2. As the simplest example, the job is not automatically checkpointed with a timed schedule, but only does so\n   when receiving the system signal from Slurm that it is about to be ended.\n   If scheduled checkpoints are still desired, return the `-i` flag to the `start_coordinator` command. \n\n3. There is only one job id, and one standard output/error file\n   associated with multiple requeued jobs. The Slurm `sacct` command accepts a\n   `--duplicates` flag which can be used to display more complete information about requeued\n   jobs.\n\nThese features are enabled with the following additional `sbatch` flags\nand a bash function `requeue_job`, which traps the signal (USR1) sent\nfrom Slurm:\n\n#SBATCH --comment=12:00:00         #maximum time available to job and all requeued jobs\n#SBATCH --signal=B:USR1@60\n#SBATCH --requeue                  #specify job is requeueable\n#SBATCH --open-mode=append         #to append standard out/err of the requeued job  \n                                   #to that of the previously terminated job\n```\n  \n```shell\n#requeueing the job if remaining time >0",
            "is_markdown": true
        }
    ],
    "12": [
        {
            "url": "https://docs.nersc.gov/jobs/troubleshooting/#runtime-errors",
            "content": "### Runtime errors\n\n-   Error message:\n\n    ```\n    srun: error: eio_handle_mainloop: Abandoning IO 60 secs after job shutdown initiated.\n    ```\n\n    Possible causes/remedies:\n\n    Slurm is giving up waiting for stdout/stderr to finish. This\n    typically happens when some rank ends early while others are\n    still wanting to write. If you don't get complete stdout/stderr\n    from the job, please resubmit the job.\n\n-   Error message:\n\n    ```\n    slurmstepd: error: _send_launch_resp: Failed to send RESPONSE_LAUNCH_TASKS: Resource temporarily unavailable\n    ```\n\n    Possible causes/remedies:\n\n    This situation does not affect the job. This issue may have been fixed.\n\n-   Error message:\n\n    ```\n    srun: fatal: Can not execute vasp_gam\n    /var/spool/slurmd/job15816716/slurm_script: line 17: 34559\n    Aborted                 srun -n 32 -c8 --cpu-bind=cores vasp_gam\n    ```\n\n    Possible causes/remedies:\n\n    The user does not belong to a VASP group. The user needs to\n    [provide VASP license info following the instructions](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/applications/vasp/#access).\n\n-   Error message:\n\n    ```\n    $ sqs",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/applications/vasp/examples/long-time.sh",
            "content": "##                 overrun, or shared-overrun.\n## '--comment=XX'  is the total time that SUM of restarts can run (can be VERY LARGE).\n## '--time=XX'     is the maximum time that individual restart can run. This MUST fit\n##                 inside the queue limit for the QoS that you want to use\n##                 (see https://docs.nersc.gov/jobs/policy/ for details).\n## '--time-min=XX' is the minimum time that job can run before being preempted (using this\n##                 option can make it easier for Slurm to fit into queue, possibly allowing\n##                 job to start sooner). Omit this parameter unless running in either\n##                 --qos=preempt or --qos=debug_preempt.\n##                 (set this large enough to have enough time to write checkpoint file(s)\n##                  before time limit is reached; 60 seconds is usually enough).\n## '--requeue'     specifies job is elegible for requeue in case of preemption.\n## '--open-mode=append' appends contents of to the end of standard output and standard\n##                 error files with successive requeues.\n\n# Remove STOPCAR file so job isn't blocked\nrm STOPCAR\nfi\n\n# Select VASP module of choice\nmodule load vasp/5.4.4-cpu\n\n# srun must execute in background and catch signal on wait command\n# so ampersand ('&') is REQUIRED here\nsrun -n 128 -c 2 --cpu_bind=cores vasp_std &\n\n# Put any commands that need to run to continue the next job (fragment) here\nckpt_vasp() {",
            "is_markdown": false
        },
        {
            "url": "https://docs.nersc.gov/jobs/monitoring/#job-accounting",
            "content": "## Job Accounting\n\n??? example \"sacct example\"\n\n    ```\n    $ sacct      \n           JobID    JobName  Partition    Account  AllocCPUS      State ExitCode \n    ------------ ---------- ---------- ---------- ---------- ---------- -------- \n    10009775             sh  regular_m      proj1        256     FAILED      1:0 \n    10009775.ex+     extern                 proj1        256  COMPLETED      0:0 \n    10009775.0         bash                 proj1          1     FAILED      1:0 \n    10009775.1        a.out                 proj1        256  COMPLETED      0:0 \n    31171781             sh       resv      proj1        256  COMPLETED      0:0 \n    31171781.ex+     extern                 proj1        256  COMPLETED      0:0 \n    31171781.0         bash                 proj1          1  COMPLETED      0:0 \n    31172253             sh       resv      proj1        256    TIMEOUT      0:0 \n    31172253.ex+     extern                 proj1        256  COMPLETED      0:0 \n    31172253.0         bash                 proj1          1  COMPLETED      0:0 \n    ```",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/applications/vasp/examples/long-time.sh",
            "content": "#SBATCH --time-min=0:05:00\n#SBATCH --signal=B:USR1@60\n#SBATCH --requeue\n#SBATCH --open-mode=append\n\n## Notes on parameters above:\n##\n## '--qos=XX'      can be set to any of several QoS to which the user has access, which\n##                 may include: regular, debug, shared, preempt, debug-preempt, premium,\n##                 overrun, or shared-overrun.\n## '--comment=XX'  is the total time that SUM of restarts can run (can be VERY LARGE).\n## '--time=XX'     is the maximum time that individual restart can run. This MUST fit\n##                 inside the queue limit for the QoS that you want to use\n##                 (see https://docs.nersc.gov/jobs/policy/ for details).\n## '--time-min=XX' is the minimum time that job can run before being preempted (using this\n##                 option can make it easier for Slurm to fit into queue, possibly allowing\n##                 job to start sooner). Omit this parameter unless running in either\n##                 --qos=preempt or --qos=debug_preempt.\n##                 (set this large enough to have enough time to write checkpoint file(s)\n##                  before time limit is reached; 60 seconds is usually enough).\n## '--requeue'     specifies job is elegible for requeue in case of preemption.\n## '--open-mode=append' appends contents of to the end of standard output and standard\n##                 error files with successive requeues.\n\n# Remove STOPCAR file so job isn't blocked",
            "is_markdown": false
        },
        {
            "url": "https://docs.nersc.gov/jobs/troubleshooting/#troubleshooting-jobs",
            "content": "---------  ------------  -----------------  ---------  -----------\n    m0001***      750000.0          50000.0     750000.0    50000.0\n    *   = user share of project Negative\n    **  = project balance negative\n    *** = user and project balance negative\n    ```\n\n    Due to this change, project `m0001` will be restricted to subset of queues. If you have previously run jobs\n    on default queues (`debug`, `regular`), your jobs will be stuck indefinitely and you should consider killing\n\n??? \"Why is my job queued for so long?\"\n\n    The most common cause is that the job requested a long walltime limit: **queue wait time correlates strongly \n    with walltime request and weakly with number of nodes** (until you reach about half the machine).\n    \n    The reason for this is the way that jobs are scheduled. Every 5-7 minutes, Slurm makes a plan about what \n    priority order, and inserting each into the first empty slot in which it fits. After \n    making that schedule, Slurm scans down the list of jobs not on the schedule, and tries to determine \n    whether it can start each job **right now**, without having to change anything in its schedule. In other \n    words, it looks for jobs to fill gaps at the front of the schedule. This is why jobs submitted after yours \n    in the regular queue might start before your jobs.\n\n    The characteristics of jobs that are able to fit into those gaps are uniformly that they request a relatively \n    short walltime (generally 4 hours or less; less is better). Until your node count gets up to about half the",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/development/checkpoint-restart/dmtcp/#perlmutter",
            "content": "3. There is only one job id, and one standard output/error file\n   associated with multiple requeued jobs. The Slurm `sacct` command accepts a\n   `--duplicates` flag which can be used to display more complete information about requeued\n   jobs.\n\nThese features are enabled with the following additional `sbatch` flags\nand a bash function `requeue_job`, which traps the signal (USR1) sent\nfrom Slurm:\n\n#SBATCH --comment=12:00:00         #maximum time available to job and all requeued jobs\n#SBATCH --signal=B:USR1@60\n#SBATCH --requeue                  #specify job is requeueable\n#SBATCH --open-mode=append         #to append standard out/err of the requeued job  \n                                   #to that of the previously terminated job\n```\n  \n```shell\n#requeueing the job if remaining time >0\nrequeue_job func_trap USR1\n\nwait\n```\n\nwhere the `--comment` sbatch flag is used to specify the desired\nwalltime and to track the remaining walltime for the job (after\npre-termination).  You can specify any length of time, e.g., a week or\neven longer.  The `--signal` flag is used to request that the batch\nsystem sends user-defined signal USR1 to the batch shell (where the\njob is running) `sig_time` seconds (e.g., 60) before the job hits the\nwall limit.  i\n\nUpon receiving the signal USR1 from the batch system 60 seconds before\nthe job hits the wall limit, the `requeue_job` executes the following",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/development/checkpoint-restart/mana/examples/pm-cpu-mana-preempt-vt.sh",
            "content": "#!/bin/bash \n#SBATCH -J test\n#SBATCH -q preempt \n#SBATCH -N 2             \n#SBATCH -C cpu\n#SBATCH -t 24:00:00 \n#SBATCH -e %x-%j.err \n#SBATCH -o %x-%j.out\n#\n#SBATCH --comment=48:00:00\n#SBATCH --signal=B:USR1@300\n#SBATCH --requeue\n#SBATCH --open-mode=append\n\nmodule load mana nersc_cr\n\n#checkpointing once every hour\nmana_coordinator -i 3600\n\n#checkpointing/restarting jobs\nif [[ $(restart_count) == 0 ]]; then\n\n    srun -n 64 mana_launch ./a.out &\nelif [[ $(restart_count) > 0 ]] && [[ -e dmtcp_restart_script.sh ]]; then\n\n    srun -n 64 mana_restart &\nelse\n\n    echo \"Failed to restart the job, exit\"; exit\nfi\n\n# requeueing the job if remaining time >0\nckpt_command=ckpt_mana    #checkpointing additionally right before the job hits the walllimit \nrequeue_job func_trap USR1\n\nwait",
            "is_markdown": false
        },
        {
            "url": "https://docs.nersc.gov/development/checkpoint-restart/mana/#perlmutter-cpu",
            "content": "the desired walltime (48 hours in this example).   \n\n2. Optionally, each job checkpoints one more time 300 seconds before the\n   job hits the allocated time limit.   \n\n3. There is only one job ID, and one standard output/error file\n   associated with multiple shorter jobs.\n\nThese features are enabled with the following additional sbatch flags\nfrom the batch system:\n\n```slurm\n#SBATCH --comment=48:00:00         #comment for the job\n#SBATCH --signal=B:USR1@<sig_time> \n#SBATCH --requeue                  #specify job is requeueable\n#SBATCH --open-mode=append         #to append standard out/err of the requeued job  \n                                   #to that of the previously terminated job\n```\n  \n```shell\n#requeueing the job if remaining time >0\nckpt_command=ckpt_mana \n\nwait\n```\n\nwhere the `--comment` sbatch flag is used to specify the desired\nwalltime and to track the remaining walltime for the job (after\npre-termination).  You can specify any length of time, e.g., a week or\neven longer.  The `--signal` flag is used to request that the batch\nsystem sends user-defined signal USR1 to the batch shell (where the\njob is running) `sig_time` seconds (e.g., 300) before the job hits the\nwall limit.  This time should match the checkpoint overhead of your\njob.\n\nUpon receiving the signal USR1 from the batch system (300 seconds before\nthe job hits the wall limit), the `requeue_job` executes the following",
            "is_markdown": true
        }
    ],
    "13": [
        {
            "url": "https://docs.nersc.gov/jobs/workflow/scrontab/#canceling-a-scrontab-job",
            "content": "## Canceling a Scrontab job\n\nTo remove a scontab job from your running jobs you can edit the scontab\nfile with `scrontab -e` and comment out all the lines associated with \nthe entry. \n\n!!! warning \"Using `scancel` on a scontab job\"\n    The `scancel` command will give a warning when attempting \n    to remove a job started with `scrontab`. \n\n    ```bash\n    perlmutter$ scancel 555\n    scancel: error: Kill job error on job id 555: Cannot scancel a scrontab job without the --hurry flag, or modify scrontab jobs through scontrol\n    ```\n    By canceling a scontab job with the `--hurry` flag, the entry in the \n    scrontab file will be prepended with `#DISABLED`. These comments \n    will needs to be removed before the job will be able to start again.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/jobs/workflow/fireworks/#display-the-fireworks-dashboard",
            "content": "WARNING: This is a development server. Do not use it in a production deployment.\n   Use a production WSGI server instead.\n * Debug mode: off\n * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n```\n\nLeave this window running. You can `CTRL+C` to kill your FireWorks dashboard when you're done.\n\nSince you've forwarded port 5000 from Perlmutter to port 5000 on your local machine,\nyou can open a browser and navigate to\n\n`http://127.0.0.1:5000/`\n\nYou should be able to see and interact with the FireWorks dashboard tracking\nget updated job status.\n\n![screenshot_of_fireworks_dashboard](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/jobs/workflow/images/fireworks-dash.png)\n\nIf you have any questions or problems using FireWorks at NERSC, please contact\nus at [`help.nersc.gov`](https://help.nersc.gov).\n\nYou may also find it useful to reach out to the",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/applications/vasp/#running-multiple-vasp-jobs-simultaneously",
            "content": "each component job.\n\nFor example, consider the case of running 2 VASP jobs simultaneously,\neach on a single Perlmutter CPU node. One has prepared 2 input files, where\neach input resides in its own directory under a common parent directory.\nFrom the parent directory one can create a job script like below,\n\n\n\t```slurm\n\t--8<-- \"docs/applications/vasp/examples/perlmutter-cpu-mvasp-5.4.4.sh\"\n\t```\n\nthen generate a file named `joblist.in`, which contains the number of\njobs to run and the VASP run directories (one directory per line).\n\n??? example \"Sample `joblist.in` file\"\n\n\t```slurm\n\t--8<-- \"docs/applications/vasp/txt/joblist.in\"\n\t```\n\nOne can then use the script `gen_joblist.sh` that is available via the\n\n```shell\nmodule load mvasp\ngen_joblist.sh\n```\n\nThen, submit the job via `sbatch`:\n\n```shell\nsbatch run_mvasp.slurm \n```\n\n!!! note\n    * Be aware that running too many VASP jobs at once may overwhelm\n      the file system where your job is running.  Please do not run\n      jobs in your global home directory.\n    * In the sample job script above, to reduce the job startup time\n      for large jobs the executable was copied to the /tmp file system\n      (memory) of the compute nodes using the `sbcast` command prior\n      to execution.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/jobs/monitoring/#updating-jobs",
            "content": "## Updating Jobs\n\n\n### Cancel jobs\n\nTo cancel a specific job:\n\n```\nscancel $JobID\n```\n\nYou can also cancel more than one job in a single call to `scancel`:\n\n```\nscancel $JobID1 $JobID2\n```\n\nTo cancel all jobs owned by a user\n\n!!!warning\n    If you want to cancel several hundred jobs, do not perform this action as\n    one bulk change; cancel jobs by subset instead.\n   \n```\nscancel -u $USER\n```\n\nBecause `scancel` sends a remote procedure call to the Slurm daemon, a\ndegradation of service can result from many `scancel` calls happening\nall at once. Therefore we recommend using as few individual calls to\nthis function as possible. In particular, do not wrap `scancel` in a\nloop in a script or other function.\n\n\n### Change timelimit\n\n```\nscontrol update jobid=$JobID timelimit=$new_timelimit\n```\n\n\n### Change QOS\n\n```\nscontrol update jobid=$JobID qos=$new_qos\n```\n\n\n### Change account\n\n```\nscontrol update jobid=$JobID account=$new_project_to_charge\n```\n\n!!! note\n\tThe new project must be eligible to run the job.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/development/languages/python/faq-troubleshooting/#how-can-i-checkpoint-my-python-code",
            "content": "## How can I checkpoint my Python code?\n\nCheckpointing your code can make your workflow more robust to:\n\n* **System issues.** If your job crashes because of a system issue, you will be\n  able to restart the checkpointed calculation in a resubmitted job later\n  and it can pick up where it left off.\n* **User error.** The most common use case here is that the calculation takes\n  longer than the user expected when the job was submitted, and doesn't finish\n  before the time limit.\n* **Preemption.** Some HPC systems offer preemptable queues, where jobs can be run\n  with discount charging because they may be interrupted for higher priority\n  jobs. If your code can be preempted because it can checkpoint, you can take\n  advantage of discount charging or submit shorter jobs. The net effect may be\n  actually faster throughput for your workflow.\n\nThis example\n[repo](https://gitlab.com/NERSC/checkpoint-on-signal-example/-/tree/master/python)\ndemonstrates one simple way to add graceful error handling and checkpointing to a Python\ncode. Note, mpi4py jobs should generally be run with srun on NERSC systems. For example:\n\n```shell\nsrun -n 2 ./main.py\n```\n\nis suitable for checkpointing. For checkpointing to work, other Python jobs must be run with exec:\n\n```shell\nexec ./main.py\n```\n\nso that the `SIGINT` signal will be forwarded. (Bash will not do this.) The InterruptHandler\nclass in this\n[example](https://gitlab.com/NERSC/checkpoint-on-signal-example/-/blob/master/python/main.py)",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/systems/perlmutter/running-jobs/#oversubscribing-gpus-with-cuda-multi-process-service",
            "content": "Then, you can launch your application as usual by using an `srun` command.\n\nTo shut down the MPS control daemon and revert back to the default CUDA runtime, run:\n\n```\necho quit | nvidia-cuda-mps-control\n```\n\nFor multi-node jobs, the MPS control daemon must be started on each node\nbefore running your application. One way to accomplish this is to use a wrapper script\ninserted after the `srun` portion of the command:\n\n```bash\n#!/bin/bash\n# Example mps-wrapper.sh usage:\n# > srun [srun args] mps-wrapper.sh [cmd] [cmd args]\nexport CUDA_MPS_PIPE_DIRECTORY=/tmp/nvidia-mps\nexport CUDA_MPS_LOG_DIRECTORY=/tmp/nvidia-log\n# Launch MPS from a single rank per node\nif [ $SLURM_LOCALID -eq 0 ]; then\n    CUDA_VISIBLE_DEVICES=$SLURM_JOB_GPUS nvidia-cuda-mps-control -d\nfi\n# Wait for MPS to start\n# Run the command\n\"$@\"\n# Quit MPS control daemon before exiting\nif [ $SLURM_LOCALID -eq 0 ]; then\n    echo quit | nvidia-cuda-mps-control\nfi\n```\n\nFor this wrapper script to work, all GPUs per node must be visible to node local rank 0\nso it is unlikely to work in conjunction with Slurm options that restrict access to GPUs",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/connect/mfa/#q-i-have-two-devices-how-do-i-copy-or-transfer-my-token-from-one-to-the-other",
            "content": "### (Q) I have two devices. How do I copy or transfer my token from one to the other?\n\nIf you are using Google Authenticator, then follow \n[Google's instructions](https://support.google.com/accounts/answer/1066447)\nunder the section \"Transfer Google Authenticator codes to a new phone\".\n\nAlternatively, when Iris generates a QR code along with the\n\"secret\" code (see the 'Creating and Installing a Token' section),\nyou can create a token on each device using the same QR or secret\ncode, if you want. Then, if multiple devices' internal clocks are\nrunning at the same rate and the time on the devices is the same,\nthe authenticator apps on the multiple devices will show the identical\nOTP.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/jobs/workflow/taskfarmer/#step-2-create-a-task-list-taskstxt",
            "content": "### Step 2: Create a task list (`tasks.txt`)\n\nThis is where you can list all the tasks you need, including all job\noptions.\n\n```\n$SCRATCH/taskfarmer/wrapper.sh 0 0 1\n$SCRATCH/taskfarmer/wrapper.sh 0 1 0\n$SCRATCH/taskfarmer/wrapper.sh 0 1 1\n$SCRATCH/taskfarmer/wrapper.sh 1 0 0\n$SCRATCH/taskfarmer/wrapper.sh 1 0 1\n$SCRATCH/taskfarmer/wrapper.sh 1 1 1\n```",
            "is_markdown": true
        }
    ],
    "14": [
        {
            "url": "https://docs.nersc.gov/applications/vasp/examples/perlmutter-cpu-v5.sh",
            "content": "#!/bin/bash\n#SBATCH -N 2\n#SBATCH -C cpu\n#SBATCH -q regular\n#SBATCH -t 01:00:00\n#SBATCH -J vasp_job\n#SBATCH -o %x-%j.out\n#SBATCH -e %x-%j.err\n\n# Default version loaded: vasp/5.4.4-cpu\nmodule load vasp\n\n# Run with (-n) 256 total MPI ranks\n#  128-MPI-ranks-per-node is maximum on Perlmutter CPU\n# Set -c (\"--cpus-per-task\") = 2 \n#  to space processes two \"logical cores\" apart\nsrun -n 256 -c 2 --cpu-bind=cores vasp_std",
            "is_markdown": false
        },
        {
            "url": "https://docs.nersc.gov/development/checkpoint-restart/mana/#perlmutter-cpu",
            "content": "#### Perlmutter CPU\n\n??? example \"`run.slurm`: the job you wish to checkpoint\"  \n\n    ```slurm\n    --8<-- \"docs/development/checkpoint-restart/mana/examples/pm-cpu.sh\"\n    ```\n\n??? example \"`run_launch.slurm`: launches your job under MANA control\"  \n\n    ```slurm\n    --8<-- \"docs/development/checkpoint-restart/mana/examples/pm-cpu-mana-preempt.sh\"\n    ```\n\n??? example \"`run_restart.slurm`: restarts your job from checkpoint files with MANA\"  \n\n    ```slurm\n    --8<-- \"docs/development/checkpoint-restart/mana/examples/pm-cpu-mana-restart-preempt.sh\"\n    ```\n\nTo **run** the job, just submit the C/R job scripts above,  \n\n```shell\nsbatch run_launch.slurm\nsbatch run_restart.slurm   #if the first job is pre-terminated\nsbatch run_restart.slurm   #if the second job is pre-terminated\n...\n```\n\nThe first job will run with a specified time limit of 24 hours. If it is\npreempted before then, you will\nneed to submit the restart job, `run_restart.slurm`.  You may need to\nsubmit it multiple times until the job completes or has run for 24\nhours as requested.  You can use the",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/applications/vasp/#sample-job-scripts",
            "content": "### Sample Job Scripts\n\nTo run batch jobs, prepare a job script (see samples\nbelow), and submit it to the batch system with the `sbatch`\ncommand, e.g. for job script named `run.slurm`,\n\n```shell\nnersc$ sbatch run.slurm\n```\n\nPlease check the [Queue Policy](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/jobs/policy/) page for the\navailable QOS settings and their resource limits.\n \n\n#### Perlmutter GPUs \n\n??? example \"Sample job script for running VASP 6 on Perlmutter GPU nodes\"\n\n\t```slurm\n\t--8<-- \"docs/applications/vasp/examples/perlmutter-gpu.sh\"\n\t```\n\n\n#### Perlmutter CPUs\n\n??? example \"Sample job script for running VASP 5 on Perlmutter CPU nodes\"\n\n\t```slurm\n\t--8<-- \"docs/applications/vasp/examples/perlmutter-cpu-v5.sh\"\n\t```\n\n??? example \"Sample job script for running VASP 6 on Perlmutter CPU nodes\"\n\n\t```slurm\n\t--8<-- \"docs/applications/vasp/examples/perlmutter-cpu-v6.sh\"\n\t```",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/applications/vasp/examples/long-time.sh",
            "content": "##                 overrun, or shared-overrun.\n## '--comment=XX'  is the total time that SUM of restarts can run (can be VERY LARGE).\n## '--time=XX'     is the maximum time that individual restart can run. This MUST fit\n##                 inside the queue limit for the QoS that you want to use\n##                 (see https://docs.nersc.gov/jobs/policy/ for details).\n## '--time-min=XX' is the minimum time that job can run before being preempted (using this\n##                 option can make it easier for Slurm to fit into queue, possibly allowing\n##                 job to start sooner). Omit this parameter unless running in either\n##                 --qos=preempt or --qos=debug_preempt.\n##                 (set this large enough to have enough time to write checkpoint file(s)\n##                  before time limit is reached; 60 seconds is usually enough).\n## '--requeue'     specifies job is elegible for requeue in case of preemption.\n## '--open-mode=append' appends contents of to the end of standard output and standard\n##                 error files with successive requeues.\n\n# Remove STOPCAR file so job isn't blocked\nrm STOPCAR\nfi\n\n# Select VASP module of choice\nmodule load vasp/5.4.4-cpu\n\n# srun must execute in background and catch signal on wait command\n# so ampersand ('&') is REQUIRED here\nsrun -n 128 -c 2 --cpu_bind=cores vasp_std &\n\n# Put any commands that need to run to continue the next job (fragment) here\nckpt_vasp() {",
            "is_markdown": false
        },
        {
            "url": "https://docs.nersc.gov/development/languages/python/parallel-python/#launching-programs",
            "content": "### Launching programs\n\nSince multiprocessing jobs are single node only, in general you will not\nneed Slurm `srun`. You can run your jobs on a compute node via\n\n```shell\npython my_multiprocessing_script.py\n```\n\n!!! Warning \"When using Slurm `srun`, use `--cpu-bind=none` for multiprocessing\"\n    If you do want or need to use Slurm `srun`, make sure you set\n\n    ```shell\n    srun -n 1 --cpu-bind=none python my_multiprocessing_script.py\n    ```\n    \n    to ensure that your single task is able to use all cores on the\n    node. Note that this is different than the advice you may get from\n    our [NERSC jobscript generator](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/jobs/jobscript-generator/)\n    as this configuration is somewhat unusual. Using `--cpu-bind=cores` will\n    bind your single task to a single physical core instead of allowing\n    your multiprocessing code to use all resources on the node.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/jobs/workflow/taskfarmer/#step-3-create-a-batch-script-submittaskfarmersl",
            "content": "### Step 3: Create a batch script (`submit_taskfarmer.sl`)\n\nYour batch script will specify the total time required to run all your\ntasks and how many nodes you want to use.  Assuming you are running on\nthe Perlmutter CPU partition, note that you need to specify the `-c 128`\noption to ensure that all 128 cores are available to the\nworkers, and `export THREADS=128` to ensure that each task runs on one\nthread.  The `-N 2` requests two compute nodes - one will run the\nTaskFarmer server, and the other will run the tasks.  `tasks.txt` is the\ntasklist you created in the previous step.  The `THREADS` variable controls\nthe number of tasks per node and defaults to 128 for Perlmutter CPU nodes.\nYou may need to adjust this number if, for example, your application\nrequires additional memory or uses internal threading (e.g. OpenMP or\nPOSIX threads).\n\n!!! Warning \"You must load the TaskFarmer module\"\n    `module load taskfarmer` before you submit your job!\n\n```slurm\n#!/bin/sh\n#SBATCH -N 2 -c 128\n#SBATCH -q debug\n#SBATCH -t 00:05:00\n#SBATCH -C cpu\n\ncd $SCRATCH/taskfarmer\nexport THREADS=128\n\nruncommands.sh tasks.txt\n```",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/applications/vasp/examples/long-time.sh",
            "content": "#!/bin/bash\n#SBATCH -N 1\n#SBATCH -C cpu\n#SBATCH -J vasp_job\n#SBATCH -o %x-%j.out\n#SBATCH -e %x-%j.err\n#SBATCH --qos=debug_preempt\n#SBATCH --comment=0:45:00\n#SBATCH --time=0:05:00\n#SBATCH --time-min=0:05:00\n#SBATCH --signal=B:USR1@60\n#SBATCH --requeue\n#SBATCH --open-mode=append\n\n## Notes on parameters above:\n##\n## '--qos=XX'      can be set to any of several QoS to which the user has access, which\n##                 may include: regular, debug, shared, preempt, debug-preempt, premium,\n##                 overrun, or shared-overrun.\n## '--comment=XX'  is the total time that SUM of restarts can run (can be VERY LARGE).\n## '--time=XX'     is the maximum time that individual restart can run. This MUST fit\n##                 inside the queue limit for the QoS that you want to use\n##                 (see https://docs.nersc.gov/jobs/policy/ for details).\n## '--time-min=XX' is the minimum time that job can run before being preempted (using this\n##                 option can make it easier for Slurm to fit into queue, possibly allowing\n##                 job to start sooner). Omit this parameter unless running in either\n##                 --qos=preempt or --qos=debug_preempt.",
            "is_markdown": false
        },
        {
            "url": "https://docs.nersc.gov/jobs/examples/prepare-env/prepare-env.sh",
            "content": "#!/bin/bash -l\n\n# Submit this script as: \"./prepare-env.sh\" instead of \"sbatch prepare-env.sh\"\n\n# Prepare user env needed for Slurm batch job\n# such as module load, setup runtime environment variables, or copy input files, etc.\n# Basically, these are the commands you usually run ahead of the srun command \n\nmodule load cray-hdf5\nexport OMP_NUM_THREADS=4\n\n# Generate the Slurm batch script below with the here document, \n# then when sbatch the script later, the user env set up above will run on the login node\n# instead of on a head compute node (if included in the Slurm batch script),\n# and inherited into the batch job.\n\ncat << EOF > prepare-env.sl \n#!/bin/bash\n#SBATCH -t 30:00\n#SBATCH -N 2\n#SBATCH -q debug\n#SBATCH -C cpu\n\nsrun -n 16 -c 32 --cpu_bind=cores ./myapp.exe \n\n# Other commands needed after srun, such as copy your output filies,\n# should still be included in the Slurm script.\ncp <my_output_file> <target_location>/.\nEOF\n\n# Now submit the batch job\nsbatch prepare-env.sl",
            "is_markdown": false
        }
    ],
    "15": [
        {
            "url": "https://docs.nersc.gov/connect/#connect-to-nersc-computational-systems",
            "content": "or\n\n```\nssh <user>@saul.nersc.gov\n```\n\nIf you have configured [sshproxy](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/connect/mfa/#sshproxy) then you can run the following:\n\n```\nssh -i ~/.ssh/nersc <user>@perlmutter.nersc.gov   # or 'ssh -i ~/.ssh/nersc <user>@saul.nersc.gov'\n```\n\nThis assumes your identity file is in `~/.ssh/nersc`. The sshproxy\nroute will be convenient if you have multiple SSH connections without\nhaving to authenticate every time.\n\nYou can also connect to Perlmutter from a [DTN](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/systems/dtn/#access) and then",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/connect/mfa/#ssh-configuration-file-options",
            "content": "```\nHost perlmutter*.nersc.gov saul*.nersc.gov dtn*.nersc.gov\n    User <myusername>\n    IdentityFile ~/.ssh/nersc\n    IdentitiesOnly yes\n    ForwardAgent yes\n```\n\nWith that entry, whenever you ssh to one of those NERSC systems,\nyour ssh client will automatically use your proxy key. If your\nlocal username is the same as your NERSC username, then you do not\nneed to add the `User <myusername>` line. Otherwise substitute your\nNERSC username for `<myusername>`.\n\nIf your ssh client does not present a valid ssh key to the ssh\nserver, the server will prompt you to authenticate with NERSC\npassword + OTP. Neither the server nor the client will tell you\nthat your key has expired.\n\nAfter you set up ssh keys as above, you login with ssh to a NERSC\ncomputational machine without further authentication, as long as\nthe keys haven't expired:\n\n```console\n$ ssh perlmutter.nersc.gov\n *****************************************************************\n *                      NOTICE TO USERS                          *\n *                      ---------------                          *\n...\n$                    # You're on Perlmutter\n```\n\nAfter you log in, you can build your code, submit batch jobs, debug\nyour code, etc. as you would normally do on any login node.\n\nYou can transfer a file to or from a NERSC machine with scp, in the\nsame manner as you use ssh:\n\n```console\n$ scp myfile perlmutter.nersc.gov:~\n *****************************************************************\n *                                                               *\n *                      NOTICE TO USERS                          *",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/connect/mfa/#ssh-configuration-file-options",
            "content": "##### SSH Configuration File Options\n\nWe recommend some options to put in your ssh config file. These\noptions help avoid some potential problems with expiring ssh keys,\nand provide default key filenames to ssh so that you don't have to\nspecify the key on the command line every time you use ssh. These\noptions can all be overridden on the command-line at any time.\n\nIf you typically use only the default `nersc` key from sshproxy,\nyou can modify your ssh config file to automatically use that key,\ninstead of having to specify it on the command line every time. To\ndo so, edit the file `~/.ssh/config` on your local computer to\ninclude the following lines:\n\n```\nHost perlmutter*.nersc.gov saul*.nersc.gov dtn*.nersc.gov\n    User <myusername>\n    IdentityFile ~/.ssh/nersc\n    IdentitiesOnly yes\n    ForwardAgent yes\n```\n\nWith that entry, whenever you ssh to one of those NERSC systems,\nyour ssh client will automatically use your proxy key. If your\nlocal username is the same as your NERSC username, then you do not\nneed to add the `User <myusername>` line. Otherwise substitute your\nNERSC username for `<myusername>`.\n\nIf your ssh client does not present a valid ssh key to the ssh\nserver, the server will prompt you to authenticate with NERSC\npassword + OTP. Neither the server nor the client will tell you\nthat your key has expired.\n\nAfter you set up ssh keys as above, you login with ssh to a NERSC\ncomputational machine without further authentication, as long as\nthe keys haven't expired:\n\n```console\n$ ssh perlmutter.nersc.gov\n *****************************************************************",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/jobs/monitoring/#how-to-log-into-compute-nodes-running-your-jobs",
            "content": "## How to log into compute nodes running your jobs\n\nIt can be useful for troubleshooting or diagnostics to log into compute nodes\nrunning one's job in order to observe activity on those nodes. Below is the\nseries of steps required to log into a compute node while one's job is running.\n\n!!! Note \"Access to compute nodes is enabled only while the job is running\"\n    A user's SSH access to compute nodes is enabled only during the lifetime of\n    the job.  When the job ends, the user's SSH connections to all compute\n    nodes in the job will be disconnected.\n\n1. Retrieve the list of nodes that your job is running on. This will either\nprint the host name `nid*****` or a range of host names -- if the job has more\nthan one node -- in square brackets.\n\n    ```\n    scontrol show job <jobid> | grep -oP  'NodeList=nid(\\[.+\\]|.+)'\n    ```\n\n2. SSH into any `nid*****` node in the `scontrol`\nlist generated in step 1.\n\n!!! Note \"Requesting the head-node ID\"\n    If you need the head-node only (eg. for DMTCP applications) use `BatchHost`\n    instead of `NodeList`:\n\n    ```\n    scontrol show job <jobid>|grep -oP 'BatchHost=\\K\\w+'\n    ```",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/accounts/collaboration_accounts/#logging-into-collaboration-accounts",
            "content": "## Logging Into Collaboration Accounts\n\n\n### Direct Login\n\nYou can ssh directly to any NERSC system as your collaboration account\nby using [sshproxy](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/connect/mfa/#sshproxy) to generate an ssh key\nfor the account\n\n```console\n$ sshproxy.sh -c your_collab_acct\nEnter the password+OTP for elvis: \nSuccessfully obtained ssh key /Users/elvis/.ssh/your_collab_acct\nKey /Users/elvis/.ssh/your_collab_acct is valid: from 2019-07-16T09:46:00 to 2019-07-17T09:47:58\n```\n\nYou can use this key to ssh (or scp, etc.) to any NERSC system, e.g.\n\n```console\nssh -i $HOME/.ssh/your_collab_acct your_collab_acct@perlmutter.nersc.gov\n```\n\n\n### Indirect Login\n\nYou can also access your collaboration account if you've already\nlogged into a data transfer node (DTN),\nusing:\n\n```console\n$ collabsu <collaboration account name>\n<enter nersc password at the prompt>\n```\n\n!!! tip\n    Only your NERSC password is required for collabsu. You do not need\n    your one-time password.\n\n!!! warning \"Please note, `collabsu` is not currently available on Perlmutter.\"",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/connect/nx/#configure-a-nersc-connection",
            "content": "1. Click the yellow Connect button at the top right. Enter your NERSC username.\n    1. Verify the key. Create a new desktop. You should land at the NERSC homescreen.\n    1. You can connect to Perlmutter via the green buttons on the left hand menu.\n       GNOME desktop. If you click `x` without logging out, your session may hang\n       which will prevent you from starting a new session. If you think this has\n       happened to you, please open a ticket at `help.nersc.gov` so we can help\n       you reset your NoMachine access.\n    1. To use this connection in the future, when you start the NoMachine client, just\n       click in the icon you named \"Connection to NERSC\".\n\n??? example \"Authenticate with password and OTC\"\n\n    1. Open the NoMachine client and click on the green Add icon in the upper left corner\n    1. Under Address > Name the connection something like \"Connection to NERSC\"\n    1. Under Address > Protocol set it to `ssh`. This should automatically change the Port to 22.\n    1. Under Configuration > Authentication, select \"Use password authentication\"\n    1. Click the yellow Connect button in the top right corner\n    1. Type your NERSC username and password+OTP. Don't save your password in the connection file; you will need to enter a new OTP every time you log on.\n    1. Verify the key. Create a new desktop You should land at the NERSC homescreen.\n    1. You can connect to Perlmutter via the green buttons on the left hand menu.\n    1. To safely log out, click the `log out` button on the left hand menu of the",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/connect/nx/#configure-a-nersc-connection",
            "content": "and click the back error at the top left.\n    1. Click the yellow Connect button at the top right. Enter your NERSC username.\n    1. Verify the key. Create a new desktop. You should land at the NERSC homescreen.\n    1. You can connect to Perlmutter via the green buttons on the left hand menu.\n       GNOME desktop. If you click `x` without logging out, your session may hang\n       which will prevent you from starting a new session. If you think this has\n       happened to you, please open a ticket at the [NERSC Help Desk](https://help.nersc.gov)\n       so we can help you reset your NoMachine access.\n    1. To use this connection in the future, when you start the NoMachine client, just\n       click in the icon you named \"Connection to NERSC\".\n\n??? example \"Authenticate via sshproxy (Windows)\"\n\n       config files on your local machine. **You must edit this file while NoMachine is\n       closed/not running**. Make sure you can view hidden files and\n       navigate to `C:\\Users\\<yourusername>\\.nx\\config\\player.cfg`. You can edit this file\n       using Notepad++ or other programs that allow you to edit hidden files\n       which cannot be edited by default in Windows.\n       Change the following key from `library` to `native`: `<option key=\"SSH client mode\" value=\"native\" />`\n    1. [Install sshproxy on your laptop and generate an SSH key](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/connect/mfa/#sshproxy).\n       Note that you must do this once every day to generate a new key.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/connect/#connecting-with-ssh",
            "content": "### Connecting with SSH\n\nOn a system with an SSH client installed, one can log into the\nPerlmutter login nodes by running the following command:\n\n```\nssh <username>@perlmutter.nersc.gov   # or 'ssh <username>@saul.nersc.gov'\n```\n\nIf the user has generated a temporary SSH key using\n[`sshproxy`](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/connect/mfa/#sshproxy), then this command will connect the user to one\nof Perlmutter's login nodes. If the user has not generated a temporary SSH key, then\nSSH will challenge the user for their [Iris](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/iris/iris-for-users/)\npassword as well as their [one-time password\n(OTP)](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/connect/mfa/#configuring-and-using-an-mfa-token) before connecting them to a\nlogin node.\n\nUsers are strongly encouraged to use the most up-to-date versions of SSH\nclients that are available.",
            "is_markdown": true
        }
    ],
    "16": [
        {
            "url": "https://docs.nersc.gov/policies/resource-usage/#running-out-of-allocation",
            "content": "## Running out of Allocation\n\nAccounting information for the previous day is finalized in Iris once daily (in \nthe early morning, Pacific Time). At this time actions are taken if a project or \nuser balance is negative.\n\nIf a project runs out of time (or space in HPSS) all login names which are not \nassociated with another active project are restricted:\n\n*  On computational machines, restricted users are able to log in, but cannot \nsubmit batch jobs or run parallel jobs, except to the \"overrun\" partition. \n*  For HPSS, restricted users are able to read data from HPSS and delete files, \nbut cannot write any data to HPSS.\n\nLogin names that are associated with more than one project (for a given \nresource -- compute or HPSS) are checked to see if the user has a positive \nbalance in any of their projects (for that resource). If they do have a positive \nbalance (for that resource), they will not be restricted and the following will \nhappen:\n\n*  On computational machines the user will not be able to charge to the \nrestricted project. If the restricted project had been the user's default \nproject, they will need to change their default project through Iris, or specify\na different project with sufficient allocation when submitting a job, or run \njobs in overrun only. \n\nLikewise, when a user goes over their individual user quota in a given \nproject, that user is restricted if they have no other project to charge to. A \nPI or Project Manager can change the user's quota.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/jobs/troubleshooting/#troubleshooting-jobs",
            "content": "check our [queue policy](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/jobs/policy/) for list of available queues and correct\n    your job script.\n\n    If your account incurs a negative balance, your project will be restricted to **overrun** and **xfer** queue.\n    Take for example, project `m0001` has a negative balance since it used 75,000 node hours whereas the\n\n    ```console\n    $ iris\n    Project      Used(user)    Allocated(user)       Used    Allocated\n    ---------  ------------  -----------------  ---------  -----------\n    m0001***      750000.0          50000.0     750000.0    50000.0\n    *   = user share of project Negative\n    **  = project balance negative\n    *** = user and project balance negative\n    ```\n\n    Due to this change, project `m0001` will be restricted to subset of queues. If you have previously run jobs\n    on default queues (`debug`, `regular`), your jobs will be stuck indefinitely and you should consider killing\n\n??? \"Why is my job queued for so long?\"\n\n    The most common cause is that the job requested a long walltime limit: **queue wait time correlates strongly \n    with walltime request and weakly with number of nodes** (until you reach about half the machine).\n    \n    The reason for this is the way that jobs are scheduled. Every 5-7 minutes, Slurm makes a plan about what",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/jobs/troubleshooting/#troubleshooting-jobs",
            "content": "---------  ------------  -----------------  ---------  -----------\n    m0001***      750000.0          50000.0     750000.0    50000.0\n    *   = user share of project Negative\n    **  = project balance negative\n    *** = user and project balance negative\n    ```\n\n    Due to this change, project `m0001` will be restricted to subset of queues. If you have previously run jobs\n    on default queues (`debug`, `regular`), your jobs will be stuck indefinitely and you should consider killing\n\n??? \"Why is my job queued for so long?\"\n\n    The most common cause is that the job requested a long walltime limit: **queue wait time correlates strongly \n    with walltime request and weakly with number of nodes** (until you reach about half the machine).\n    \n    The reason for this is the way that jobs are scheduled. Every 5-7 minutes, Slurm makes a plan about what \n    priority order, and inserting each into the first empty slot in which it fits. After \n    making that schedule, Slurm scans down the list of jobs not on the schedule, and tries to determine \n    whether it can start each job **right now**, without having to change anything in its schedule. In other \n    words, it looks for jobs to fill gaps at the front of the schedule. This is why jobs submitted after yours \n    in the regular queue might start before your jobs.\n\n    The characteristics of jobs that are able to fit into those gaps are uniformly that they request a relatively \n    short walltime (generally 4 hours or less; less is better). Until your node count gets up to about half the",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/jobs/examples/#projects-that-have-exhausted-their-allocation",
            "content": "## Projects That Have Exhausted Their Allocation\n\nA project with zero or negative NERSC-hours balance can submit to the \nthe overrun queue.\n\nIf you meet the \n[overrun criteria](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/policies/resource-usage/#overrun),\nyou can access the overrun queue by\nsubmitting with `-q overrun` (`-q shared_overrun` for the shared\nqueue). On Perlmutter, all overrun jobs require the `--time-min` flag at job\nsubmission and are subject to preemption by higher priority workloads under\ncertain circumstances.\n\n!!! tip\n    We recommend you implement\n    [checkpoint/restart](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/development/checkpoint-restart/)\n    your overrun jobs to save your progress.\n\n!!! example\n    A job requesting a minimum time of 1.5 hours:\n\n    ```\n    sbatch -q overrun --time-min=01:30:00 my_batch_script.sl\n    ```",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/development/checkpoint-restart/mana/#perlmutter-cpu",
            "content": "```\n\n??? example \"`run_restart.slurm`: restarts your job from checkpoint files with MANA\"  \n\n    ```slurm\n    --8<-- \"docs/development/checkpoint-restart/mana/examples/pm-cpu-mana-restart-preempt.sh\"\n    ```\n\nTo **run** the job, just submit the C/R job scripts above,  \n\n```shell\nsbatch run_launch.slurm\nsbatch run_restart.slurm   #if the first job is pre-terminated\nsbatch run_restart.slurm   #if the second job is pre-terminated\n...\n```\n\nThe first job will run with a specified time limit of 24 hours. If it is\npreempted before then, you will\nneed to submit the restart job, `run_restart.slurm`.  You may need to\nsubmit it multiple times until the job completes or has run for 24\nhours as requested.  You can use the \nyour C/R jobs at once (you may need to submit many more restart jobs\nthan actually needed). You can also combine the two C/R job scripts into one\n(see the next section), and then submit it multiple times as dependent\njobs all at once.  However, this is still not as convenient as\nsubmitting the job script `run.slurm` only once.  The good news is\nthat you can automate the C/R jobs using the features supported in\nSlurm and a trap function (see the next section). The job scripts in\nthe next section are recommended to run C/R jobs.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/development/checkpoint-restart/mana/#perlmutter-cpu",
            "content": "#### Perlmutter CPU\n\n??? example \"`run.slurm`: the job you wish to checkpoint\"  \n\n    ```slurm\n    --8<-- \"docs/development/checkpoint-restart/mana/examples/pm-cpu.sh\"\n    ```\n\n??? example \"`run_launch.slurm`: launches your job under MANA control\"  \n\n    ```slurm\n    --8<-- \"docs/development/checkpoint-restart/mana/examples/pm-cpu-mana-preempt.sh\"\n    ```\n\n??? example \"`run_restart.slurm`: restarts your job from checkpoint files with MANA\"  \n\n    ```slurm\n    --8<-- \"docs/development/checkpoint-restart/mana/examples/pm-cpu-mana-restart-preempt.sh\"\n    ```\n\nTo **run** the job, just submit the C/R job scripts above,  \n\n```shell\nsbatch run_launch.slurm\nsbatch run_restart.slurm   #if the first job is pre-terminated\nsbatch run_restart.slurm   #if the second job is pre-terminated\n...\n```\n\nThe first job will run with a specified time limit of 24 hours. If it is\npreempted before then, you will\nneed to submit the restart job, `run_restart.slurm`.  You may need to\nsubmit it multiple times until the job completes or has run for 24\nhours as requested.  You can use the",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/applications/vasp/examples/long-time.sh",
            "content": "#SBATCH --time-min=0:05:00\n#SBATCH --signal=B:USR1@60\n#SBATCH --requeue\n#SBATCH --open-mode=append\n\n## Notes on parameters above:\n##\n## '--qos=XX'      can be set to any of several QoS to which the user has access, which\n##                 may include: regular, debug, shared, preempt, debug-preempt, premium,\n##                 overrun, or shared-overrun.\n## '--comment=XX'  is the total time that SUM of restarts can run (can be VERY LARGE).\n## '--time=XX'     is the maximum time that individual restart can run. This MUST fit\n##                 inside the queue limit for the QoS that you want to use\n##                 (see https://docs.nersc.gov/jobs/policy/ for details).\n## '--time-min=XX' is the minimum time that job can run before being preempted (using this\n##                 option can make it easier for Slurm to fit into queue, possibly allowing\n##                 job to start sooner). Omit this parameter unless running in either\n##                 --qos=preempt or --qos=debug_preempt.\n##                 (set this large enough to have enough time to write checkpoint file(s)\n##                  before time limit is reached; 60 seconds is usually enough).\n## '--requeue'     specifies job is elegible for requeue in case of preemption.\n## '--open-mode=append' appends contents of to the end of standard output and standard\n##                 error files with successive requeues.\n\n# Remove STOPCAR file so job isn't blocked",
            "is_markdown": false
        },
        {
            "url": "https://docs.nersc.gov/applications/vasp/examples/long-time.sh",
            "content": "##                 overrun, or shared-overrun.\n## '--comment=XX'  is the total time that SUM of restarts can run (can be VERY LARGE).\n## '--time=XX'     is the maximum time that individual restart can run. This MUST fit\n##                 inside the queue limit for the QoS that you want to use\n##                 (see https://docs.nersc.gov/jobs/policy/ for details).\n## '--time-min=XX' is the minimum time that job can run before being preempted (using this\n##                 option can make it easier for Slurm to fit into queue, possibly allowing\n##                 job to start sooner). Omit this parameter unless running in either\n##                 --qos=preempt or --qos=debug_preempt.\n##                 (set this large enough to have enough time to write checkpoint file(s)\n##                  before time limit is reached; 60 seconds is usually enough).\n## '--requeue'     specifies job is elegible for requeue in case of preemption.\n## '--open-mode=append' appends contents of to the end of standard output and standard\n##                 error files with successive requeues.\n\n# Remove STOPCAR file so job isn't blocked\nrm STOPCAR\nfi\n\n# Select VASP module of choice\nmodule load vasp/5.4.4-cpu\n\n# srun must execute in background and catch signal on wait command\n# so ampersand ('&') is REQUIRED here\nsrun -n 128 -c 2 --cpu_bind=cores vasp_std &\n\n# Put any commands that need to run to continue the next job (fragment) here\nckpt_vasp() {",
            "is_markdown": false
        }
    ],
    "17": [
        {
            "url": "https://docs.nersc.gov/performance/vectorization/#data-dependency",
            "content": "### Data dependency\n\n\n#### read-after-write\n\nAlso known as \"flow dependency\". Vectorization creates wrong results.\n\n```fortran\ndo i=2,n\n\ta(i) = a(i-1) + 1\nend do\n```\n\n\n#### write-after-read\n\nAlso known as \"anti-dependency\" and can be vectorized.\n\n```fortran\ndo i=2,n\n\ta(i-1) = a(i) + 1\nend do\n```\n\n\n#### write-after-write\n\nAlso known as \"output dependency\" and cannot be vectorized.\n\n```fortran\ndo i=2,n\n\ta(i-1) = x(i)\n\ta(i)   = 2.0 * i\nend do\n```\n\n\n#### Reduction operations\n\nReduction operations can be vectorized.\n\n```fortran\ns=0.0\ndo i=1,n\n\ts = s + a(i) * b(i)\nend do\n```",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/applications/vasp/#sample-job-scripts",
            "content": "### Sample Job Scripts\n\nTo run batch jobs, prepare a job script (see samples\nbelow), and submit it to the batch system with the `sbatch`\ncommand, e.g. for job script named `run.slurm`,\n\n```shell\nnersc$ sbatch run.slurm\n```\n\nPlease check the [Queue Policy](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/jobs/policy/) page for the\navailable QOS settings and their resource limits.\n \n\n#### Perlmutter GPUs \n\n??? example \"Sample job script for running VASP 6 on Perlmutter GPU nodes\"\n\n\t```slurm\n\t--8<-- \"docs/applications/vasp/examples/perlmutter-gpu.sh\"\n\t```\n\n\n#### Perlmutter CPUs\n\n??? example \"Sample job script for running VASP 5 on Perlmutter CPU nodes\"\n\n\t```slurm\n\t--8<-- \"docs/applications/vasp/examples/perlmutter-cpu-v5.sh\"\n\t```\n\n??? example \"Sample job script for running VASP 6 on Perlmutter CPU nodes\"\n\n\t```slurm\n\t--8<-- \"docs/applications/vasp/examples/perlmutter-cpu-v6.sh\"\n\t```",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/services/sfapi/examples/#batch-job-submission",
            "content": "system = \"perlmutter\"\nsubmit_script = \"<path to script>\" # (e.g., /global/homes/u/username/script.sub)\nr = session.post(\"https://api.nersc.gov/api/v1.2/compute/jobs/\"+system,\n                  data = {\"job\": submit_script, \"isPath\": True})\nprint(r.json())\n```\n\nThe result output will include the SF API task ID, the status of the\n`POST` request, and any error text:\n\n```json\n{'task_id': '124', 'status': 'ok', 'error': None}\n```\n\n!!! warning \"`POST` request status does not indicate job submission status\"\n    The result output `status` is that of the `POST` request **only** and does\n    not indicate whether or not the job submission was accepted by the Slurm\n    scheduler.\n\n!!! danger \"This feature requires elevated client permissions\"",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/tutorials/playbooks/running/#step-4-write-a-batch-script",
            "content": "### Step 4: Write a batch script\n\nNext we will encorporate all of the above considerations about how to run the \napplication into our procedure.\n\nMost applications should be run in batch (asynchronous) mode. The majority of \napplications do not require any human intervention or feedback. So a job that \ncan run asynchronously can be run at any time, including times when you are \nasleep, spending time with family, or performing another task -- a major win \nfor convenience!\n\nNERSC has extensive documentation on [batch scripts](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/jobs/),\nincluding many [example job scripts](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/jobs/examples/). Another\ngreat resource is the [job script generator](https://iris.nersc.gov/jobscript),\nwhich can be used to ensure correct process affinities and other settings.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/applications/vasp/examples/long-time.sh",
            "content": "#SBATCH --time-min=0:05:00\n#SBATCH --signal=B:USR1@60\n#SBATCH --requeue\n#SBATCH --open-mode=append\n\n## Notes on parameters above:\n##\n## '--qos=XX'      can be set to any of several QoS to which the user has access, which\n##                 may include: regular, debug, shared, preempt, debug-preempt, premium,\n##                 overrun, or shared-overrun.\n## '--comment=XX'  is the total time that SUM of restarts can run (can be VERY LARGE).\n## '--time=XX'     is the maximum time that individual restart can run. This MUST fit\n##                 inside the queue limit for the QoS that you want to use\n##                 (see https://docs.nersc.gov/jobs/policy/ for details).\n## '--time-min=XX' is the minimum time that job can run before being preempted (using this\n##                 option can make it easier for Slurm to fit into queue, possibly allowing\n##                 job to start sooner). Omit this parameter unless running in either\n##                 --qos=preempt or --qos=debug_preempt.\n##                 (set this large enough to have enough time to write checkpoint file(s)\n##                  before time limit is reached; 60 seconds is usually enough).\n## '--requeue'     specifies job is elegible for requeue in case of preemption.\n## '--open-mode=append' appends contents of to the end of standard output and standard\n##                 error files with successive requeues.\n\n# Remove STOPCAR file so job isn't blocked",
            "is_markdown": false
        },
        {
            "url": "https://docs.nersc.gov/jobs/examples/#example",
            "content": "### Example\n\nSample job script for MPMD jobs. You need to create a configuration\nfile with format described above, and a batch script which passes this\nconfiguration file via `--multi-prog` flag in the srun command.\n\n!!! example \"Perlmutter CPU\"\n\t```slurm\n    --8<-- \"docs/jobs/examples/mpmd/perlmutter-cpu/mpmd\"\n    ```",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/applications/vasp/examples/long-time.sh",
            "content": "##                 overrun, or shared-overrun.\n## '--comment=XX'  is the total time that SUM of restarts can run (can be VERY LARGE).\n## '--time=XX'     is the maximum time that individual restart can run. This MUST fit\n##                 inside the queue limit for the QoS that you want to use\n##                 (see https://docs.nersc.gov/jobs/policy/ for details).\n## '--time-min=XX' is the minimum time that job can run before being preempted (using this\n##                 option can make it easier for Slurm to fit into queue, possibly allowing\n##                 job to start sooner). Omit this parameter unless running in either\n##                 --qos=preempt or --qos=debug_preempt.\n##                 (set this large enough to have enough time to write checkpoint file(s)\n##                  before time limit is reached; 60 seconds is usually enough).\n## '--requeue'     specifies job is elegible for requeue in case of preemption.\n## '--open-mode=append' appends contents of to the end of standard output and standard\n##                 error files with successive requeues.\n\n# Remove STOPCAR file so job isn't blocked\nrm STOPCAR\nfi\n\n# Select VASP module of choice\nmodule load vasp/5.4.4-cpu\n\n# srun must execute in background and catch signal on wait command\n# so ampersand ('&') is REQUIRED here\nsrun -n 128 -c 2 --cpu_bind=cores vasp_std &\n\n# Put any commands that need to run to continue the next job (fragment) here\nckpt_vasp() {",
            "is_markdown": false
        },
        {
            "url": "https://docs.nersc.gov/services/sfapi/examples/#batch-job-submission",
            "content": "### Batch Job Submission\n\nHere, we demonstrate submitting a job to the\nSlurm scheduler via the SF API. The system name, `perlmutter`, is\nplaced in the URL path, following the API base path and the `/compute/jobs`\npath. Additionally, a `data` dictionary of key-value pairs is included in the\n`POST` request. The `data` dictionary specifies the job submission script path\nas the value to the `job` key and the value of `True` to the `isPath` key to\nindicate that the file should be sent to the Slurm scheduler.\n\n```python\nsystem = \"perlmutter\"\nsubmit_script = \"<path to script>\" # (e.g., /global/homes/u/username/script.sub)\nr = session.post(\"https://api.nersc.gov/api/v1.2/compute/jobs/\"+system,\n                  data = {\"job\": submit_script, \"isPath\": True})\nprint(r.json())\n```\n\nThe result output will include the SF API task ID, the status of the\n`POST` request, and any error text:\n\n```json\n{'task_id': '124', 'status': 'ok', 'error': None}\n```\n\n!!! warning \"`POST` request status does not indicate job submission status\"\n    The result output `status` is that of the `POST` request **only** and does\n    not indicate whether or not the job submission was accepted by the Slurm\n    scheduler.\n\n!!! danger \"This feature requires elevated client permissions\"",
            "is_markdown": true
        }
    ],
    "18": [
        {
            "url": "https://docs.nersc.gov/connect/mfa/#q-mfa-is-enabled-for-my-account-but-i-didnt-set-a-mfa-token-on-an-authenticator-app-how-can-i-log-into-iris-to-set-it-up",
            "content": "### (Q) MFA is enabled for my account but I didn't set a MFA token on an authenticator app. How can I log into Iris to set it up?\n\nPlease follow the instructions in the ['If You Lost Your Tokens'\nsection](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/connect/#if-you-lost-your-tokens), to get an OTP that you will use\nto login into your Iris account in order to setup a MFA token.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/accounts/passwords/#multi-factor-authentication",
            "content": "## Multi-Factor Authentication\n\nMulti-factor authentication (MFA) is required for user accounts. NERSC\nusers must set up an MFA token and log into NERSC resources using their\npassword plus a six-digit number generated by the MFA token. Like a \npassword, an MFA token must be kept secure and not shared with others.\n\nThe page on [Multi-Factor Authentication](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/connect/mfa/) explains\nMFA in more detail and includes information on setting up your MFA\ntoken and answers to common MFA questions.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/iris/iris-for-users/#reset-mfa-tokens",
            "content": "#### Reset MFA tokens\n\nIf you have to reset all your MFA tokens because, for example, you\nlost the device where MFA tokens were configured, please follow the\ninstructions in the ['If You Lost Your Tokens' section of the MFA\nwebpage](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/connect/mfa/#if-you-lost-your-tokens).",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/applications/wrf/wrf/#example-wrf-build-script-for-perlmutter",
            "content": "#### Example WRF build script for Perlmutter\n\n```bash\n#!/bin/bash\nset -e\nset -o pipefail \n\n#change the following boolean variables to run/skip certain compiling steps\n\ndoclean=false  #true if WRF source code is modified since the last compilation\n\ndoclean_all=false  #true if previously compiled with different configure options\n\nrunconf=true    #run WRF's configure script; should do this first before compiling\n\ndocompile=true  #run WRF's compile script; should do this after configure\n\ndebug=false  #true to compile WRF with debug flag (no optimizations, -g flag for debugger, etc.)\n\nimach=\"pm\"  #target system name. \"pm\" for Perlmutter.\n\n# set the top directory of the WRF source code as an environmental variable\nexport WRF_DIR=\"PATH_TO_YOUR_WRFcode_LOCATION\"\n\n#Modules --------------------------------------------------------------------\n#general modules\nmodule load cpu\t \nmodule load PrgEnv-gnu \n\n#module for WRF file I/O\n#order of loading matters!\nmodule load cray-hdf5  #required to load netcdf library\nmodule load cray-netcdf\t\nmodule load cray-parallel-netcdf\n    \nmodule list #check what modules are loaded\n\n#set environmental variables used by WRF build system, \n#using the environmental variables set by the modules\n\n#use classic (CDF1) as default\nexport NETCDF_classic=1 \n#use 64-bit offset format (CDF2) of netcdf files",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/connect/mfa/#if-you-lost-your-tokens",
            "content": "### If You Lost Your Tokens\n\nIf your MFA tokens are lost permanently (for example, you replaced\nyour mobile device) or if you are a first-time user and didn't\ncomplete the MFA token configuration process before logging out of\nIris, you can request a one-time password that can be used for\nyou to log into Iris for setting up an MFA token. Please follow the\nsteps below.\n\n1.  Click the '**MFA not working?**' link on the [Iris login\n    page](https://iris.nersc.gov).\n\n2.  Enter your username and password. Click OK.\n\n    ![Iris: Lost your tokens, username and password](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/connect/images/iris_lost_mfatoken_01.png)\n    {: align=\"center\"}\n\n3.  A dialog box shows up and asks if you want to create a MFA token. Click OK.\n\n    ![Iris: Lost your tokens, confirmation](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/connect/images/iris_lost_mfatoken_02.png)\n    {: align=\"center\"}\n\n4.  A new dialog box will show up.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/connect/mfa/#configuring-and-using-an-mfa-token",
            "content": "## Configuring and Using an MFA Token\n\nThe one-time password entry in the authenticator app is sometimes\ncalled a \"token,\" or more specifically, a \"soft token.\" To use MFA,\nyou create a token for NERSC and install it on the authenticator\napp.\n\nThe four basic steps for configuring your NERSC token are:\n\n```mermaid\nflowchart LR\n  first[Install the authenticator app]\n  second[Enable MFA in your Iris account]\n  third[Generate a NERSC OTP token via Iris]\n  fourth[Install the token on the authenticator app]\n  first --> second --> third --> fourth\n```",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/connect/mfa/#if-you-lost-your-tokens",
            "content": "1.  Click the '**MFA not working?**' link on the [Iris login\n    page](https://iris.nersc.gov).\n\n2.  Enter your username and password. Click OK.\n\n    ![Iris: Lost your tokens, username and password](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/connect/images/iris_lost_mfatoken_01.png)\n    {: align=\"center\"}\n\n3.  A dialog box shows up and asks if you want to create a MFA token. Click OK.\n\n    ![Iris: Lost your tokens, confirmation](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/connect/images/iris_lost_mfatoken_02.png)\n    {: align=\"center\"}\n\n4.  A new dialog box will show up.\n\n    {: align=\"center\" style=\"border:1px solid black\"}\n\n5.  If you have entered the correct password, NERSC will send an\n    email. The email contains a single-use OTP.\n\n6.  Use it to login to Iris. [Create and install a new MFA\n    token](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/connect/#creating-and-installing-a-token). If your previous MFA\n    tokens are lost forever, make sure to delete them all.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/connect/mfa/#q-what-if-a-nersc-resource-doesnt-support-mfa-yet",
            "content": "### (Q) What if a NERSC resource doesn't support MFA yet?\n\nYou can login to that resource with your NERSC password only.",
            "is_markdown": true
        }
    ],
    "19": [
        {
            "url": "https://docs.nersc.gov/services/jupyter/reference/#jupyterlab-server-logs",
            "content": "## JupyterLab Server Logs\n\nLogs from login node JupyterLab usage appears in `.jupyter-$NERSC_HOST.log`\nin your home directory.\nAt start-up, if you already have one of these log files in place and it is big\n(1 GB) then it is deleted and a new one is started.\nEach new JupyterLab server you launch appends to this log file, to help with\ndebugging and retain some history.\n\nLogs from JupyterLab servers launched on compute nodes (shared or exclusive\nGPU nodes, exclusive CPU nodes, etc.) are written to `slurm-<job-id>.out` files\nthat appear in your home directory while the job is running.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/jobs/monitoring/#how-to-log-into-compute-nodes-running-your-jobs",
            "content": "## How to log into compute nodes running your jobs\n\nIt can be useful for troubleshooting or diagnostics to log into compute nodes\nrunning one's job in order to observe activity on those nodes. Below is the\nseries of steps required to log into a compute node while one's job is running.\n\n!!! Note \"Access to compute nodes is enabled only while the job is running\"\n    A user's SSH access to compute nodes is enabled only during the lifetime of\n    the job.  When the job ends, the user's SSH connections to all compute\n    nodes in the job will be disconnected.\n\n1. Retrieve the list of nodes that your job is running on. This will either\nprint the host name `nid*****` or a range of host names -- if the job has more\nthan one node -- in square brackets.\n\n    ```\n    scontrol show job <jobid> | grep -oP  'NodeList=nid(\\[.+\\]|.+)'\n    ```\n\n2. SSH into any `nid*****` node in the `scontrol`\nlist generated in step 1.\n\n!!! Note \"Requesting the head-node ID\"\n    If you need the head-node only (eg. for DMTCP applications) use `BatchHost`\n    instead of `NodeList`:\n\n    ```\n    scontrol show job <jobid>|grep -oP 'BatchHost=\\K\\w+'\n    ```",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/iris/iris-for-users/#problems-accessing-iris",
            "content": "#### Problems accessing Iris?\n\nIf you still have a problem with your username or password, contact\nthe NERSC Account Support Office by sending email to accounts@nersc.gov.\n\nIf you can't log in due to a different reason, get help from a NERSC\nconsultant by opening a trouble ticket from NERSC Help Portal\n([https://help.nersc.gov](https://help.nersc.gov)).",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/services/spin/faq/#why-are-ip-addresses-in-the-10420016-range-showing-in-my-web-service-access-log",
            "content": "## Why are IP addresses in the 10.42.0.0/16 range showing in my web service access log?\n\nWeb services in Spin are made accessible externally by an _ingress controller_, which is a web proxy. Most web services,\nby default, will therefore log accesses as originating from the IP address of the ingress controller rather than that\nof the user's workstation.\n\nTo change this behavior, configure your web service to obtain the IP address from the `X-Forwarded-For` HTTP header,\nwhich is a convention for passing this information through web proxies. The ingress controller in Spin is already\nconfigured to populate the IP address into the `X-Forwarded-For` HTTP header.\n\nConfiguration for Apache:\n\n```\nLoadModule remoteip_module modules/mod_remoteip.so\nRemoteIPHeader X-Forwarded-For\nRemoteIPInternalProxy 10.42.0.0/16\nRemoteIPInternalProxy 128.55.137.128/25\nRemoteIPInternalProxy 128.55.206.0/24\n```\n\nConfiguration for nginx:\n\n```\nreal_ip_header X-Forwarded-For;\nset_real_ip_from 10.42.0.0/16;\nset_real_ip_from 128.55.137.128/25;\nset_real_ip_from 128.55.206.0/24;\n```",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/connect/nx/#troubleshooting-nx-connection-failures",
            "content": "## Troubleshooting NX Connection Failures\n\nIf you are having trouble connecting to NoMachine, please try these steps first:\n\n1. Log into [Iris](https://iris.nersc.gov) to clear any login\n   failures. Access to NoMachine uses your NERSC user name and password. If\n   your password is mistyped five times, NERSC will lock you out of\n   Our systems. Logging into Iris will automatically clear these\n   failures. This will also let you know if your password is expired\n   (which would prevent you from accessing NoMachine, among many other\n   things).\n\n1. Create a new connection file following [the instructions\n   above](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/connect/#configure-a-nersc-connection). NX will often\n   \"update\" the configuration file to try to save your settings and\n   occasionally incorrect settings can be saved. You must have the\n   new NoMachine player AND an updated configuration file to connect\n   to the NoMachine service.\n\n1. Try to ssh directly to the NoMachine server. This will help to \n   distinguish if you are having a connection issue that is specific to\n   NoMachine or a general SSH connection issue. You can do this with the\n   command <br />\n   `ssh <nersc_username>@nxcloud01.nersc.gov` <br />\n   and your NERSC user name and password+one-time MFA password \n   (with no spaces in between). If your access to the NoMachine server \n   is blocked by a local firewall or something else and you can't",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/accounts/passwords/#login-failures",
            "content": "## Login Failures\n\nYour login privileges will be disabled if you have ten login failures\nwhile entering your password on a NERSC resource. You do not need\na new password in this situation. The login failures will\nautomatically clear after 5 minutes. No additional actions are\nnecessary.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/connect/mfa/#q-i-have-enabled-mfa-my-logins-fail-repeatedly-what-should-i-do",
            "content": "### (Q) I have enabled MFA. My logins fail repeatedly. What should I do?\n\nIf this is with a particular host (Perlmutter, etc.) only, then [login to\nyour Iris account](https://iris.nersc.gov/).  Select the 'Profile'\ntab, and click the 'Account Locked?' button, and click the 'Unlock\nAccount' button in a dialog box that will appear.  That will clear\nlogin failures that may have accumulated for the host. Then, try to\nlogin to the host again.\n\nIf you enter incorrect OTPs too many times, the NERSC MFA server\nlocks you out. In that case, you have to wait for 15 minutes before\nyou try again.\n\nIf you are using ssh keys generated via `sshproxy.sh` for authentication,\ncheck if the keys have expired.\n\nA popular way of using ssh key authentication is via ssh-agent, the\n\"authentication agent.\" You add an ssh private key to ssh-agent and\nit uses the key to authenticate to a remote host that has the\nmatching public key. You may be knowingly or unknowingly using this\nmethod (especially, when you use the `-a` option with `sshproxy.sh`).\nSsh-agent goes through the saved keys one by one to see if the\ncorrect key is found. If it cannot find the matching key within 6\ntries, ssh authentication fails. When you have many keys stored in\nssh-agent, including the correct one, login can fail if the correct\nkey is not selected within the first 6 tries. To see how many keys\nare stored in `ssh-agent`, run the command `ssh-add -l` on your\nlaptop/desktop. If you see many keys there, you can delete all of",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/development/languages/python/faq-troubleshooting/#why-does-my-mpi4py-time-out-or-why-is-it-so-slow",
            "content": "## Why does my `mpi4py` time out? Or why is it so slow?\n\nRunning `mpi4py` on a large number of nodes can become slow due to all\nthe metadata that must move across our filesystems. You may experience\ntimeouts that look like this:\n\n```shell\nsrun: job 33116771 has been allocated resources\nMon Aug 3 18:24:50 2020: [PE_224]:inet_connect:inet_connect: connect failed after 301 attempts\nMon Aug 3 18:24:50 2020: [PE_224]:_pmi_inet_setup:inet_connect failed\nMon Aug 3 18:24:50 2020: [PE_224]:_pmi_init:_pmi_inet_setup (full) returned -1\n[Mon Aug 3 18:24:50 2020] [c0-0c2s7n1] Fatal error in PMPI_Init_thread: Other MPI error, error stack:\nMPIR_Init_thread(537):\nMPID_Init(246).......: channel initialization failed\nMPID_Init(647).......: PMI2 init failed: 1\n```\n\n*Easy (but temporary) fix:*\n\n```shell\nexport PMI_MMAP_SYNC_WAIT_TIME=300\n```\n\nbut this doesn't fix the problem, it just gives you more time to start up.",
            "is_markdown": true
        }
    ],
    "20": [
        {
            "url": "https://docs.nersc.gov/filesystems/archive/#best-practices",
            "content": "directory deletion / failure, then you would want to store your files\nin a structure where you use `htar` to separately bundle up each\ndirectory. On the other hand, if you are archiving data files, you\nmight want to bundle things up according to month the data was taken\nor detector run characteristics, etc. The optimal size for `htar`\nbundles is between 100 GB and 2 TB, so you may need to do several `htar`\nbundles for each set depending on the size of the data. Other best practices\ndescribed in this section include:\n\n- [Grouping smaller files](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/filesystems/#group-small-files-together)\n- [Large Retrievals](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/filesystems/#order-large-retrievals)\n- [Managing larger files](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/filesystems/#avoid-very-large-files)\n- [Accessing hpss data remotely](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/filesystems/#accessing-hpss-data-remotely)",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/filesystems/archive/#best-practices",
            "content": "## Best Practices\n\nHPSS is intended for long term storage of data that is not frequently\naccessed.\n\nThe best guide for how files should be stored in HPSS is how you might\nwant to retrieve them. If you are backing up against accidental\ndirectory deletion / failure, then you would want to store your files\nin a structure where you use `htar` to separately bundle up each\ndirectory. On the other hand, if you are archiving data files, you\nmight want to bundle things up according to month the data was taken\nor detector run characteristics, etc. The optimal size for `htar`\nbundles is between 100 GB and 2 TB, so you may need to do several `htar`\nbundles for each set depending on the size of the data. Other best practices\ndescribed in this section include:\n\n- [Grouping smaller files](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/filesystems/#group-small-files-together)\n- [Large Retrievals](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/filesystems/#order-large-retrievals)\n- [Managing larger files](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/filesystems/#avoid-very-large-files)",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/filesystems/#summary",
            "content": "### Summary\n\nFile systems are configured for different purposes. Each machine has\naccess to at least three different file systems with different\nlevels of performance, data persistence and available capacity, and\neach file system is designed to be accessed and used either by a\nuser individually or by their project, as reported in the \"Access\"\ncolumn.\n\n!!! warning\n    NERSC storage systems are architected to cover different needs \n    for performance, capacity and data persistance, and **not** as\n    disaster-proof data storage. Other than [home backups](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/filesystems/backups/)\n    we do not keep multiple copies of data. **Do not store the *only* \n    copy of your data at NERSC: make sure you have at least one copy\n    at another institution or cloud service** \n\n| File System          | Snapshots | Backup | Purging | Access  |\n|----------------------|-----------|--------|---------|---------|\n| [Home]               | yes       | yes    | no      | user    |\n| [Common]             | no        | no     | no      | project |\n| [Community]          | yes       | no     | no      | project |\n| [Perlmutter scratch] | no        | no     | yes     | user    |\n| [HPSS]               | no        | no     | no      | user    |\n\n[Home]: #home\n[Common]: #common\n[Community]: #community\n[Perlmutter scratch]: #scratch\n[HPSS]: #archive-hpss",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/performance/io/dvs/#read-your-data-from-the-right-place",
            "content": "### Read Your Data From the Right Place\n\nIf your job reads large volumes of data, the fastest file system will\nalmost always be [Perlmutter\nScratch](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/filesystems/perlmutter-scratch/). However, if many of\nthe processes in your jobs repeatedly read in the same file (e.g. a\nconfiguration file), you may see a large speedup by using a read-only\nDVS mount. On Perlmutter CFS has a corresponding read-only mount at\n`/dvs_ro/cfs`, respectively. We recommend using this for data that is\nbeing read during a job that is not being actively changed. The DVS\nmount of this file system will cache data for 30 seconds by default,\nso if data is being changed, you may see unexpected results.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/performance/io/#best-practices-for-scientific-io",
            "content": "* Use file systems for their intended use-case; for example, don't use\n your home directory for production I/O (more details on intended use\n case may be found [on the NERSC file systems\n page](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/filesystems/)).\n* Know what fraction of your wall-clock time is spent in I/O; for\n  example, with estimates provided by\n  [Darshan](https://www.mcs.anl.gov/research/projects/darshan/),\n  profiling of critical I/O routines (such as with\n  [Craypat](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/tools/performance/craypat/)'s\n  trace groups), or explicit timing / instrumentation.\n* When algorithmically possible:\n    * Avoid workflows that produce large numbers of small files\n      (e.g. a \"file-per-process\" access model at high levels of\n      concurrency).\n    * Avoid random-access I/O workloads in favor of contiguous access.\n    * Prefer I/O workloads that perform large transfers that are\n      underlying file system storage granularity (e.g. blocksize on\n      GPFS-based file systems, stripe width on Lustre-based file\n      systems). If using Fortran list-directed I/O, consider using \n      runtime environment variables to increase the read buffer size\n      (e.g. `FORT_FMT_RECL` for the Intel Fortran runtime).",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/performance/io/#best-practices-for-scientific-io",
            "content": "## Best Practices for Scientific I/O\n\nWhile there is clearly a wide range of I/O workloads associated with\nthe many scientific applications deployed at NERSC, there are a number\nof general guidelines for achieving good performance when accessing\nour file systems from parallel codes. Some of the most important\nguidelines include:\n\n* Use file systems for their intended use-case; for example, don't use\n your home directory for production I/O (more details on intended use\n case may be found [on the NERSC file systems\n page](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/filesystems/)).\n* Know what fraction of your wall-clock time is spent in I/O; for\n  example, with estimates provided by\n  [Darshan](https://www.mcs.anl.gov/research/projects/darshan/),\n  profiling of critical I/O routines (such as with\n  [Craypat](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/tools/performance/craypat/)'s\n  trace groups), or explicit timing / instrumentation.\n* When algorithmically possible:\n    * Avoid workflows that produce large numbers of small files\n      (e.g. a \"file-per-process\" access model at high levels of\n      concurrency).\n    * Avoid random-access I/O workloads in favor of contiguous access.\n    * Prefer I/O workloads that perform large transfers that are",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/filesystems/#local-storage",
            "content": "### Local storage\n\nThe following file systems provide high I/O performance, but often\ndon't preserve data across different jobs, so they are meant to be\nused as scratch space, and data produced must be staged out at the\nend of the computation.\n\nAccess is always per-user, since these file systems only\naccessible within the same SLURM job (XFS and in-RAM file systems),\nsince SLURM purges the content afterwards.\n\n\n#### Temporary per-node Shifter file system\n\nShifter users can access a fast, per-node [xfs file\nsystem](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/development/containers/shifter/how-to-use/#temporary-xfs-files-for-optimizing-io)\nto improve I/O.\n\n\n#### Local temporary file system\n\nCompute nodes have a small amount of\n[temporary local storage](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/performance/io/dev-shm/)\nthat can be used to improve I/O.",
            "is_markdown": true
        },
        {
            "url": "https://docs.nersc.gov/services/globus/#transfer-files-from-nerscs-community-file-systemhttpsdocsnerscgovglobalcfscdirsnstaffchatbotproductioncodedatanerscdocdocsfilesystemscommunity-to-nerscs-perlmutter-scratch-file-systemhttpsdocsnerscgovglobalcfscdirsnstaffchatbotproductioncodedatanerscdocdocsfilesystemsperlmutter-scratch",
            "content": "#### Transfer files from [NERSC's Community file system](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/filesystems/community/) to [NERSC's Perlmutter Scratch file system](https://docs.nersc.gov//global/cfs/cdirs/nstaff/chatbot/production_code/data/nersc_doc/docs/filesystems/perlmutter-scratch/)\n\n!!! tip\n    This can be used to stage data on Perlmutter scratch before using it in\n    a running job. See the script `stage_data.script` (included in the\n    globus-tools module) for an example of how to do this.\n\nFirst, generate a list of files and directories you wish\nto transfer. If a directory is included in this list, its contents\nwill be recursively transferred to the target directory.\n\n```\nls /global/cfs/cdirs/<myrepo>/<my_dataset_directory> > transfer.txt\nls /global/cfs/cdirs/<myrepo>/<my_other_dataset_directory>/data01.dat >> transfer.txt\n```\n\nThen invoke the transfer script\n\n```\nmodule load globus-tools\ntransfer_files.py -s dtn -t perlmutter -d /pscratch/sd/<letter>/<your_username>/input_directory -i transfer.txt\n```\n\nIf this is the first time running the script, you'll see the next two lines, followed by the\nstandard transfer information:\n\n```",
            "is_markdown": true
        }
    ]
}